* 
* ==> Audit <==
* |------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |   User    | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|
| start      |                                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 11:09 EDT |                     |
| start      | --driver=docker                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 11:46 EDT |                     |
| start      | --driver=docker                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 11:47 EDT |                     |
| start      | --driver=hyperv                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 11:47 EDT |                     |
| start      | --driver=docker                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 12:01 EDT |                     |
| start      | --driver=hyperv                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 12:02 EDT |                     |
| start      | --driver=hyperv                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 12:02 EDT |                     |
| start      |                                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 12:05 EDT |                     |
| start      | --driver=docker dekstop        | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 12:15 EDT |                     |
| delete     |                                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 12:15 EDT | 17 Sep 22 12:15 EDT |
| start      | --driver=docker dekstop        | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 12:16 EDT | 17 Sep 22 12:58 EDT |
| start      |                                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 12:59 EDT |                     |
| docker-env |                                | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 15:21 EDT | 17 Sep 22 15:21 EDT |
| docker-env | minikube docker-env --shell    | minikube | HINA\Hina | v1.27.0 | 17 Sep 22 15:22 EDT | 17 Sep 22 15:22 EDT |
|            | powershell                     |          |           |         |                     |                     |
| stop       |                                | minikube | HINA\Hina | v1.27.0 | 18 Sep 22 14:15 EDT | 18 Sep 22 14:15 EDT |
| delete     |                                | minikube | HINA\Hina | v1.27.0 | 18 Sep 22 14:16 EDT | 18 Sep 22 14:16 EDT |
| start      | --driver=docker-dekstop        | minikube | HINA\Hina | v1.27.0 | 18 Sep 22 14:17 EDT |                     |
| start      | --driver=docker dekstop        | minikube | HINA\Hina | v1.27.0 | 18 Sep 22 14:17 EDT | 18 Sep 22 14:18 EDT |
| docker-env |                                | minikube | HINA\Hina | v1.27.0 | 18 Sep 22 14:21 EDT | 18 Sep 22 14:21 EDT |
| docker-env | minikube docker-env --shell    | minikube | HINA\Hina | v1.27.0 | 18 Sep 22 14:22 EDT | 18 Sep 22 14:22 EDT |
|            | powershell                     |          |           |         |                     |                     |
| service    | spring-boot-k8s --url          | minikube | HINA\Hina | v1.27.0 | 18 Sep 22 15:43 EDT | 18 Sep 22 15:46 EDT |
| dashboard  |                                | minikube | HINA\Hina | v1.27.0 | 18 Sep 22 15:46 EDT |                     |
|------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/09/18 14:17:34
Running on machine: Hina
Binary: Built with gc go1.19.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0918 14:17:34.400435   46752 out.go:296] Setting OutFile to fd 88 ...
I0918 14:17:34.401609   46752 out.go:343] TERM=,COLORTERM=, which probably does not support color
I0918 14:17:34.401609   46752 out.go:309] Setting ErrFile to fd 92...
I0918 14:17:34.401609   46752 out.go:343] TERM=,COLORTERM=, which probably does not support color
I0918 14:17:34.401609   46752 oci.go:572] shell is pointing to dockerd inside minikube. will unset to use host
I0918 14:17:34.432776   46752 out.go:303] Setting JSON to false
I0918 14:17:34.449453   46752 start.go:115] hostinfo: {"hostname":"Hina","uptime":135730,"bootTime":1663389324,"procs":276,"os":"windows","platform":"Microsoft Windows 10 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.19044 Build 19044","kernelVersion":"10.0.19044 Build 19044","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"4686931d-c921-46e4-8acc-e07b32871636"}
W0918 14:17:34.449453   46752 start.go:123] gopshost.Virtualization returned error: not implemented yet
I0918 14:17:34.451725   46752 out.go:177] * minikube v1.27.0 on Microsoft Windows 10 Home 10.0.19044 Build 19044
I0918 14:17:34.453405   46752 notify.go:214] Checking for updates...
I0918 14:17:34.455345   46752 out.go:177]   - MINIKUBE_ACTIVE_DOCKERD=minikube
W0918 14:17:34.457340   46752 out.go:239] ! Kubernetes 1.25.0 has a known issue with resolv.conf. minikube is using a workaround that should work for most use cases.
W0918 14:17:34.458430   46752 out.go:239] ! For more information, see: https://github.com/kubernetes/kubernetes/issues/112135
I0918 14:17:34.459077   46752 driver.go:365] Setting default libvirt URI to qemu:///system
I0918 14:17:34.720032   46752 docker.go:137] docker version: linux-20.10.17
I0918 14:17:34.728681   46752 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0918 14:17:37.163376   46752 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.434695s)
I0918 14:17:37.163924   46752 info.go:265] docker info: {ID:DLUW:263C:TVB4:ALAT:LIT4:KQSD:5O4L:WB6Z:H4X5:QTGA:AHDS:5YCC Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:58 SystemTime:2022-09-18 18:17:34.9001022 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:8 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:9950257152 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.7.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0918 14:17:37.168279   46752 out.go:177] * Using the docker driver based on user configuration
I0918 14:17:37.169371   46752 start.go:284] selected driver: docker
I0918 14:17:37.169878   46752 start.go:808] validating driver "docker" against <nil>
I0918 14:17:37.169921   46752 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0918 14:17:37.191753   46752 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0918 14:17:37.890060   46752 info.go:265] docker info: {ID:DLUW:263C:TVB4:ALAT:LIT4:KQSD:5O4L:WB6Z:H4X5:QTGA:AHDS:5YCC Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:58 SystemTime:2022-09-18 18:17:37.4065999 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:8 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:9950257152 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.7.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0918 14:17:37.890686   46752 start_flags.go:296] no existing cluster config was found, will generate one from the flags 
I0918 14:17:37.956038   46752 start_flags.go:377] Using suggested 3000MB memory alloc based on sys=12204MB, container=9489MB
I0918 14:17:37.956579   46752 start_flags.go:835] Wait components to verify : map[apiserver:true system_pods:true]
I0918 14:17:37.958182   46752 out.go:177] * Using Docker Desktop driver with root privileges
I0918 14:17:37.959800   46752 cni.go:95] Creating CNI manager for ""
I0918 14:17:37.959800   46752 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0918 14:17:37.959800   46752 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Hina:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0918 14:17:37.962124   46752 out.go:177] * Starting control plane node minikube in cluster minikube
I0918 14:17:37.964266   46752 cache.go:120] Beginning downloading kic base image for docker with docker
I0918 14:17:37.965393   46752 out.go:177] * Pulling base image ...
I0918 14:17:37.967000   46752 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I0918 14:17:37.967548   46752 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon
I0918 14:17:37.968098   46752 preload.go:148] Found local preload: C:\Users\Hina\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4
I0918 14:17:37.968098   46752 cache.go:57] Caching tarball of preloaded images
I0918 14:17:37.968886   46752 preload.go:174] Found C:\Users\Hina\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0918 14:17:37.969397   46752 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.0 on docker
I0918 14:17:37.969397   46752 profile.go:148] Saving config to C:\Users\Hina\.minikube\profiles\minikube\config.json ...
I0918 14:17:37.970412   46752 lock.go:35] WriteFile acquiring C:\Users\Hina\.minikube\profiles\minikube\config.json: {Name:mk8051c9ec0cc22ed00321bd6a189e38964b304f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:17:38.230259   46752 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon, skipping pull
I0918 14:17:38.230259   46752 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c exists in daemon, skipping load
I0918 14:17:38.230259   46752 cache.go:208] Successfully downloaded all kic artifacts
I0918 14:17:38.230819   46752 start.go:364] acquiring machines lock for minikube: {Name:mk5164bab074bdfefd600fed9b34f603d0e4b1ad Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0918 14:17:38.231383   46752 start.go:368] acquired machines lock for "minikube" in 563.6Âµs
I0918 14:17:38.231383   46752 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Hina:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name: IP: Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0918 14:17:38.231964   46752 start.go:125] createHost starting for "" (driver="docker")
I0918 14:17:38.238036   46752 out.go:204] * Creating docker container (CPUs=2, Memory=3000MB) ...
I0918 14:17:38.239110   46752 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0918 14:17:38.239670   46752 client.go:168] LocalClient.Create starting
I0918 14:17:38.240207   46752 main.go:134] libmachine: Reading certificate data from C:\Users\Hina\.minikube\certs\ca.pem
I0918 14:17:38.248335   46752 main.go:134] libmachine: Decoding PEM data...
I0918 14:17:38.248335   46752 main.go:134] libmachine: Parsing certificate...
I0918 14:17:38.248935   46752 main.go:134] libmachine: Reading certificate data from C:\Users\Hina\.minikube\certs\cert.pem
I0918 14:17:38.257962   46752 main.go:134] libmachine: Decoding PEM data...
I0918 14:17:38.257962   46752 main.go:134] libmachine: Parsing certificate...
I0918 14:17:38.271674   46752 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0918 14:17:38.510643   46752 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0918 14:17:38.520334   46752 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I0918 14:17:38.520334   46752 cli_runner.go:164] Run: docker network inspect minikube
W0918 14:17:38.774145   46752 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0918 14:17:38.774145   46752 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I0918 14:17:38.774145   46752 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I0918 14:17:38.783697   46752 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0918 14:17:39.089124   46752 network.go:290] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc0004e0818] misses:0}
I0918 14:17:39.089634   46752 network.go:236] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I0918 14:17:39.089717   46752 network_create.go:115] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0918 14:17:39.097246   46752 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0918 14:17:40.057490   46752 network_create.go:99] docker network minikube 192.168.49.0/24 created
I0918 14:17:40.057490   46752 kic.go:106] calculated static IP "192.168.49.2" for the "minikube" container
I0918 14:17:40.078345   46752 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0918 14:17:40.426878   46752 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0918 14:17:40.722422   46752 oci.go:103] Successfully created a docker volume minikube
I0918 14:17:40.738258   46752 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib
I0918 14:17:43.221371   46752 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib: (2.4829212s)
I0918 14:17:43.221371   46752 oci.go:107] Successfully prepared a docker volume minikube
I0918 14:17:43.221371   46752 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I0918 14:17:43.221371   46752 kic.go:179] Starting extracting preloaded images to volume ...
I0918 14:17:43.240024   46752 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Hina\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir
I0918 14:18:01.871104   46752 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Hina\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.25.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: (18.6310802s)
I0918 14:18:01.871104   46752 kic.go:188] duration metric: took 18.649733 seconds to extract preloaded images to volume
I0918 14:18:01.887102   46752 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0918 14:18:02.630881   46752 info.go:265] docker info: {ID:DLUW:263C:TVB4:ALAT:LIT4:KQSD:5O4L:WB6Z:H4X5:QTGA:AHDS:5YCC Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:58 SystemTime:2022-09-18 18:18:02.1844808 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:8 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:9950257152 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1 Expected:10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1} RuncCommit:{ID:v1.1.2-0-ga916309 Expected:v1.1.2-0-ga916309} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.7.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0918 14:18:02.638479   46752 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0918 14:18:03.223075   46752 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3000mb --memory-swap=3000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c
I0918 14:18:05.634629   46752 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3000mb --memory-swap=3000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c: (2.4113098s)
I0918 14:18:05.657365   46752 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0918 14:18:05.952917   46752 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 14:18:06.276624   46752 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0918 14:18:06.843914   46752 oci.go:144] the created container "minikube" has a running status.
I0918 14:18:06.843914   46752 kic.go:210] Creating ssh key for kic: C:\Users\Hina\.minikube\machines\minikube\id_rsa...
I0918 14:18:07.225211   46752 kic_runner.go:191] docker (temp): C:\Users\Hina\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0918 14:18:07.758285   46752 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 14:18:08.073360   46752 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0918 14:18:08.073360   46752 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0918 14:18:08.469447   46752 kic.go:250] ensuring only current user has permissions to key file located at : C:\Users\Hina\.minikube\machines\minikube\id_rsa...
I0918 14:18:10.276160   46752 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 14:18:10.558781   46752 machine.go:88] provisioning docker machine ...
I0918 14:18:10.558781   46752 ubuntu.go:169] provisioning hostname "minikube"
I0918 14:18:10.568926   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:10.786103   46752 main.go:134] libmachine: Using SSH client type: native
I0918 14:18:10.792886   46752 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13da860] 0x13dd7e0 <nil>  [] 0s} 127.0.0.1 61022 <nil> <nil>}
I0918 14:18:10.792886   46752 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0918 14:18:11.000457   46752 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I0918 14:18:11.011103   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:11.229621   46752 main.go:134] libmachine: Using SSH client type: native
I0918 14:18:11.229621   46752 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13da860] 0x13dd7e0 <nil>  [] 0s} 127.0.0.1 61022 <nil> <nil>}
I0918 14:18:11.229621   46752 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0918 14:18:11.410453   46752 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0918 14:18:11.410498   46752 ubuntu.go:175] set auth options {CertDir:C:\Users\Hina\.minikube CaCertPath:C:\Users\Hina\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Hina\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Hina\.minikube\machines\server.pem ServerKeyPath:C:\Users\Hina\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Hina\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Hina\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Hina\.minikube}
I0918 14:18:11.410498   46752 ubuntu.go:177] setting up certificates
I0918 14:18:11.410498   46752 provision.go:83] configureAuth start
I0918 14:18:11.422855   46752 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 14:18:11.735417   46752 provision.go:138] copyHostCerts
I0918 14:18:11.735977   46752 exec_runner.go:144] found C:\Users\Hina\.minikube/ca.pem, removing ...
I0918 14:18:11.735977   46752 exec_runner.go:207] rm: C:\Users\Hina\.minikube\ca.pem
I0918 14:18:11.736530   46752 exec_runner.go:151] cp: C:\Users\Hina\.minikube\certs\ca.pem --> C:\Users\Hina\.minikube/ca.pem (1070 bytes)
I0918 14:18:11.738172   46752 exec_runner.go:144] found C:\Users\Hina\.minikube/cert.pem, removing ...
I0918 14:18:11.738172   46752 exec_runner.go:207] rm: C:\Users\Hina\.minikube\cert.pem
I0918 14:18:11.738769   46752 exec_runner.go:151] cp: C:\Users\Hina\.minikube\certs\cert.pem --> C:\Users\Hina\.minikube/cert.pem (1115 bytes)
I0918 14:18:11.746845   46752 exec_runner.go:144] found C:\Users\Hina\.minikube/key.pem, removing ...
I0918 14:18:11.746845   46752 exec_runner.go:207] rm: C:\Users\Hina\.minikube\key.pem
I0918 14:18:11.747393   46752 exec_runner.go:151] cp: C:\Users\Hina\.minikube\certs\key.pem --> C:\Users\Hina\.minikube/key.pem (1675 bytes)
I0918 14:18:11.748544   46752 provision.go:112] generating server cert: C:\Users\Hina\.minikube\machines\server.pem ca-key=C:\Users\Hina\.minikube\certs\ca.pem private-key=C:\Users\Hina\.minikube\certs\ca-key.pem org=Hina.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0918 14:18:12.034740   46752 provision.go:172] copyRemoteCerts
I0918 14:18:12.049841   46752 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0918 14:18:12.055990   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:12.263501   46752 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61022 SSHKeyPath:C:\Users\Hina\.minikube\machines\minikube\id_rsa Username:docker}
I0918 14:18:12.399232   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0918 14:18:12.445726   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0918 14:18:12.495652   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0918 14:18:12.543122   46752 provision.go:86] duration metric: configureAuth took 1.1326237s
I0918 14:18:12.543122   46752 ubuntu.go:193] setting minikube options for container-runtime
I0918 14:18:12.544227   46752 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I0918 14:18:12.556284   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:12.792501   46752 main.go:134] libmachine: Using SSH client type: native
I0918 14:18:12.792501   46752 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13da860] 0x13dd7e0 <nil>  [] 0s} 127.0.0.1 61022 <nil> <nil>}
I0918 14:18:12.792501   46752 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0918 14:18:12.980359   46752 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0918 14:18:12.980359   46752 ubuntu.go:71] root file system type: overlay
I0918 14:18:12.981503   46752 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0918 14:18:12.992608   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:13.195199   46752 main.go:134] libmachine: Using SSH client type: native
I0918 14:18:13.195755   46752 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13da860] 0x13dd7e0 <nil>  [] 0s} 127.0.0.1 61022 <nil> <nil>}
I0918 14:18:13.195755   46752 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0918 14:18:13.401326   46752 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0918 14:18:13.411883   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:13.646224   46752 main.go:134] libmachine: Using SSH client type: native
I0918 14:18:13.646224   46752 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13da860] 0x13dd7e0 <nil>  [] 0s} 127.0.0.1 61022 <nil> <nil>}
I0918 14:18:13.646224   46752 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0918 14:18:16.953214   46752 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-09-18 18:18:13.385503000 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0918 14:18:16.954213   46752 machine.go:91] provisioned docker machine in 6.3944329s
I0918 14:18:16.954213   46752 client.go:171] LocalClient.Create took 38.7145429s
I0918 14:18:16.954213   46752 start.go:167] duration metric: libmachine.API.Create for "minikube" took 38.7151031s
I0918 14:18:16.954213   46752 start.go:300] post-start starting for "minikube" (driver="docker")
I0918 14:18:16.954213   46752 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0918 14:18:16.975834   46752 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0918 14:18:16.987275   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:17.210618   46752 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61022 SSHKeyPath:C:\Users\Hina\.minikube\machines\minikube\id_rsa Username:docker}
I0918 14:18:17.362306   46752 ssh_runner.go:195] Run: cat /etc/os-release
I0918 14:18:17.372759   46752 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0918 14:18:17.372759   46752 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0918 14:18:17.372759   46752 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0918 14:18:17.372759   46752 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I0918 14:18:17.372759   46752 filesync.go:126] Scanning C:\Users\Hina\.minikube\addons for local assets ...
I0918 14:18:17.373894   46752 filesync.go:126] Scanning C:\Users\Hina\.minikube\files for local assets ...
I0918 14:18:17.373894   46752 start.go:303] post-start completed in 419.6808ms
I0918 14:18:17.383895   46752 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 14:18:17.610534   46752 profile.go:148] Saving config to C:\Users\Hina\.minikube\profiles\minikube\config.json ...
I0918 14:18:17.631565   46752 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0918 14:18:17.637215   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:17.856741   46752 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61022 SSHKeyPath:C:\Users\Hina\.minikube\machines\minikube\id_rsa Username:docker}
I0918 14:18:18.000429   46752 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0918 14:18:18.016974   46752 start.go:128] duration metric: createHost completed in 39.78501s
I0918 14:18:18.016974   46752 start.go:83] releasing machines lock for "minikube", held for 39.7855915s
I0918 14:18:18.025407   46752 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 14:18:18.247128   46752 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0918 14:18:18.255342   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:18.255934   46752 ssh_runner.go:195] Run: systemctl --version
I0918 14:18:18.267688   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:18.527646   46752 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61022 SSHKeyPath:C:\Users\Hina\.minikube\machines\minikube\id_rsa Username:docker}
I0918 14:18:18.546084   46752 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61022 SSHKeyPath:C:\Users\Hina\.minikube\machines\minikube\id_rsa Username:docker}
I0918 14:18:18.922829   46752 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0918 14:18:18.951734   46752 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0918 14:18:18.963801   46752 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0918 14:18:18.992438   46752 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0918 14:18:19.041347   46752 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0918 14:18:19.222612   46752 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0918 14:18:19.410023   46752 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0918 14:18:19.577863   46752 ssh_runner.go:195] Run: sudo systemctl restart docker
I0918 14:18:22.050582   46752 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.4721996s)
I0918 14:18:22.070846   46752 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0918 14:18:22.240777   46752 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0918 14:18:22.399817   46752 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0918 14:18:22.426249   46752 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0918 14:18:22.439692   46752 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0918 14:18:22.451199   46752 start.go:471] Will wait 60s for crictl version
I0918 14:18:22.462136   46752 ssh_runner.go:195] Run: sudo crictl version
I0918 14:18:22.671935   46752 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I0918 14:18:22.679929   46752 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0918 14:18:22.753446   46752 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0918 14:18:22.823872   46752 out.go:204] * Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
I0918 14:18:22.833870   46752 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0918 14:18:23.206490   46752 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0918 14:18:23.216724   46752 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0918 14:18:23.227204   46752 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0918 14:18:23.258850   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0918 14:18:23.470047   46752 preload.go:132] Checking if preload exists for k8s version v1.25.0 and runtime docker
I0918 14:18:23.476778   46752 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0918 14:18:23.541114   46752 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0918 14:18:23.541114   46752 docker.go:542] Images already preloaded, skipping extraction
I0918 14:18:23.553106   46752 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0918 14:18:23.624104   46752 docker.go:611] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.25.0
registry.k8s.io/kube-controller-manager:v1.25.0
registry.k8s.io/kube-scheduler:v1.25.0
registry.k8s.io/kube-proxy:v1.25.0
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0918 14:18:23.624104   46752 cache_images.go:84] Images are preloaded, skipping loading
I0918 14:18:23.637698   46752 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0918 14:18:23.775900   46752 cni.go:95] Creating CNI manager for ""
I0918 14:18:23.775900   46752 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0918 14:18:23.775900   46752 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0918 14:18:23.775900   46752 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:true}
I0918 14:18:23.775900   46752 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
resolvConf: /etc/kubelet-resolv.conf
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0918 14:18:23.776448   46752 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0918 14:18:23.789792   46752 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.0
I0918 14:18:23.808031   46752 binaries.go:44] Found k8s binaries, skipping transfer
I0918 14:18:23.821170   46752 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0918 14:18:23.840208   46752 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I0918 14:18:23.871687   46752 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0918 14:18:23.902664   46752 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2068 bytes)
I0918 14:18:23.945255   46752 ssh_runner.go:195] Run: sudo cp /etc/resolv.conf /etc/kubelet-resolv.conf
I0918 14:18:23.974256   46752 ssh_runner.go:195] Run: sudo sed -i -e "s/^search .$//" /etc/kubelet-resolv.conf
I0918 14:18:24.005412   46752 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0918 14:18:24.014902   46752 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0918 14:18:24.036451   46752 certs.go:54] Setting up C:\Users\Hina\.minikube\profiles\minikube for IP: 192.168.49.2
I0918 14:18:24.044449   46752 certs.go:182] skipping minikubeCA CA generation: C:\Users\Hina\.minikube\ca.key
I0918 14:18:24.064822   46752 certs.go:182] skipping proxyClientCA CA generation: C:\Users\Hina\.minikube\proxy-client-ca.key
I0918 14:18:24.065935   46752 certs.go:302] generating minikube-user signed cert: C:\Users\Hina\.minikube\profiles\minikube\client.key
I0918 14:18:24.065935   46752 crypto.go:68] Generating cert C:\Users\Hina\.minikube\profiles\minikube\client.crt with IP's: []
I0918 14:18:24.352086   46752 crypto.go:156] Writing cert to C:\Users\Hina\.minikube\profiles\minikube\client.crt ...
I0918 14:18:24.352086   46752 lock.go:35] WriteFile acquiring C:\Users\Hina\.minikube\profiles\minikube\client.crt: {Name:mk96c33d64c417fcfc8f289fdce28abc2fcf6b92 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:18:24.354088   46752 crypto.go:164] Writing key to C:\Users\Hina\.minikube\profiles\minikube\client.key ...
I0918 14:18:24.354088   46752 lock.go:35] WriteFile acquiring C:\Users\Hina\.minikube\profiles\minikube\client.key: {Name:mk6ae9cef9abd95787d56493aeaafcbd259b4dd9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:18:24.355088   46752 certs.go:302] generating minikube signed cert: C:\Users\Hina\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0918 14:18:24.355088   46752 crypto.go:68] Generating cert C:\Users\Hina\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0918 14:18:24.428102   46752 crypto.go:156] Writing cert to C:\Users\Hina\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 ...
I0918 14:18:24.428102   46752 lock.go:35] WriteFile acquiring C:\Users\Hina\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2: {Name:mk4f8b2967f0631b4470155bf4ea62966871f1bc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:18:24.429088   46752 crypto.go:164] Writing key to C:\Users\Hina\.minikube\profiles\minikube\apiserver.key.dd3b5fb2 ...
I0918 14:18:24.429088   46752 lock.go:35] WriteFile acquiring C:\Users\Hina\.minikube\profiles\minikube\apiserver.key.dd3b5fb2: {Name:mk396ced30e44527d8c5ab0ef7ec59c896e4b690 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:18:24.430096   46752 certs.go:320] copying C:\Users\Hina\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 -> C:\Users\Hina\.minikube\profiles\minikube\apiserver.crt
I0918 14:18:24.437088   46752 certs.go:324] copying C:\Users\Hina\.minikube\profiles\minikube\apiserver.key.dd3b5fb2 -> C:\Users\Hina\.minikube\profiles\minikube\apiserver.key
I0918 14:18:24.438090   46752 certs.go:302] generating aggregator signed cert: C:\Users\Hina\.minikube\profiles\minikube\proxy-client.key
I0918 14:18:24.438090   46752 crypto.go:68] Generating cert C:\Users\Hina\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0918 14:18:24.790271   46752 crypto.go:156] Writing cert to C:\Users\Hina\.minikube\profiles\minikube\proxy-client.crt ...
I0918 14:18:24.790271   46752 lock.go:35] WriteFile acquiring C:\Users\Hina\.minikube\profiles\minikube\proxy-client.crt: {Name:mk23702d89e00507ff26949e17f06e81d506d86d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:18:24.791247   46752 crypto.go:164] Writing key to C:\Users\Hina\.minikube\profiles\minikube\proxy-client.key ...
I0918 14:18:24.791247   46752 lock.go:35] WriteFile acquiring C:\Users\Hina\.minikube\profiles\minikube\proxy-client.key: {Name:mkf82b3878078246707cbf6c6d9c6fb788e48181 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:18:24.799245   46752 certs.go:388] found cert: C:\Users\Hina\.minikube\certs\C:\Users\Hina\.minikube\certs\ca-key.pem (1679 bytes)
I0918 14:18:24.800246   46752 certs.go:388] found cert: C:\Users\Hina\.minikube\certs\C:\Users\Hina\.minikube\certs\ca.pem (1070 bytes)
I0918 14:18:24.800246   46752 certs.go:388] found cert: C:\Users\Hina\.minikube\certs\C:\Users\Hina\.minikube\certs\cert.pem (1115 bytes)
I0918 14:18:24.800246   46752 certs.go:388] found cert: C:\Users\Hina\.minikube\certs\C:\Users\Hina\.minikube\certs\key.pem (1675 bytes)
I0918 14:18:24.801245   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0918 14:18:24.846100   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0918 14:18:24.890962   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0918 14:18:24.937221   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0918 14:18:24.980695   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0918 14:18:25.025084   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0918 14:18:25.068440   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0918 14:18:25.110106   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0918 14:18:25.178769   46752 ssh_runner.go:362] scp C:\Users\Hina\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0918 14:18:25.240862   46752 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0918 14:18:25.300406   46752 ssh_runner.go:195] Run: openssl version
I0918 14:18:25.349039   46752 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0918 14:18:25.395784   46752 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0918 14:18:25.406789   46752 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Sep 17 16:57 /usr/share/ca-certificates/minikubeCA.pem
I0918 14:18:25.420783   46752 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0918 14:18:25.453535   46752 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0918 14:18:25.478288   46752 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Hina:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0918 14:18:25.489286   46752 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0918 14:18:25.573830   46752 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0918 14:18:25.608682   46752 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0918 14:18:25.627366   46752 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I0918 14:18:25.639703   46752 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0918 14:18:25.656812   46752 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0918 14:18:25.656812   46752 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0918 14:18:25.726815   46752 kubeadm.go:317] W0918 18:18:25.724655    1120 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
I0918 14:18:25.798213   46752 kubeadm.go:317] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0918 14:18:25.936897   46752 kubeadm.go:317] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0918 14:18:53.024490   46752 kubeadm.go:317] [init] Using Kubernetes version: v1.25.0
I0918 14:18:53.024490   46752 kubeadm.go:317] [preflight] Running pre-flight checks
I0918 14:18:53.025109   46752 kubeadm.go:317] [preflight] Pulling images required for setting up a Kubernetes cluster
I0918 14:18:53.025647   46752 kubeadm.go:317] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0918 14:18:53.025661   46752 kubeadm.go:317] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0918 14:18:53.026257   46752 kubeadm.go:317] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0918 14:18:53.027388   46752 out.go:204]   - Generating certificates and keys ...
I0918 14:18:53.029714   46752 kubeadm.go:317] [certs] Using existing ca certificate authority
I0918 14:18:53.030265   46752 kubeadm.go:317] [certs] Using existing apiserver certificate and key on disk
I0918 14:18:53.030265   46752 kubeadm.go:317] [certs] Generating "apiserver-kubelet-client" certificate and key
I0918 14:18:53.030265   46752 kubeadm.go:317] [certs] Generating "front-proxy-ca" certificate and key
I0918 14:18:53.030820   46752 kubeadm.go:317] [certs] Generating "front-proxy-client" certificate and key
I0918 14:18:53.030820   46752 kubeadm.go:317] [certs] Generating "etcd/ca" certificate and key
I0918 14:18:53.030820   46752 kubeadm.go:317] [certs] Generating "etcd/server" certificate and key
I0918 14:18:53.031644   46752 kubeadm.go:317] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0918 14:18:53.031644   46752 kubeadm.go:317] [certs] Generating "etcd/peer" certificate and key
I0918 14:18:53.032217   46752 kubeadm.go:317] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0918 14:18:53.032217   46752 kubeadm.go:317] [certs] Generating "etcd/healthcheck-client" certificate and key
I0918 14:18:53.032756   46752 kubeadm.go:317] [certs] Generating "apiserver-etcd-client" certificate and key
I0918 14:18:53.032756   46752 kubeadm.go:317] [certs] Generating "sa" key and public key
I0918 14:18:53.032756   46752 kubeadm.go:317] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0918 14:18:53.033283   46752 kubeadm.go:317] [kubeconfig] Writing "admin.conf" kubeconfig file
I0918 14:18:53.033283   46752 kubeadm.go:317] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0918 14:18:53.033283   46752 kubeadm.go:317] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0918 14:18:53.033827   46752 kubeadm.go:317] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0918 14:18:53.034377   46752 kubeadm.go:317] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0918 14:18:53.034377   46752 kubeadm.go:317] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0918 14:18:53.034377   46752 kubeadm.go:317] [kubelet-start] Starting the kubelet
I0918 14:18:53.034942   46752 kubeadm.go:317] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0918 14:18:53.036581   46752 out.go:204]   - Booting up control plane ...
I0918 14:18:53.038250   46752 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0918 14:18:53.038250   46752 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0918 14:18:53.038793   46752 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0918 14:18:53.038793   46752 kubeadm.go:317] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0918 14:18:53.039424   46752 kubeadm.go:317] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0918 14:18:53.039980   46752 kubeadm.go:317] [apiclient] All control plane components are healthy after 20.005017 seconds
I0918 14:18:53.040507   46752 kubeadm.go:317] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0918 14:18:53.040540   46752 kubeadm.go:317] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0918 14:18:53.041091   46752 kubeadm.go:317] [upload-certs] Skipping phase. Please see --upload-certs
I0918 14:18:53.041630   46752 kubeadm.go:317] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0918 14:18:53.041630   46752 kubeadm.go:317] [bootstrap-token] Using token: vznqg4.re0jqsffszs6m015
I0918 14:18:53.044622   46752 out.go:204]   - Configuring RBAC rules ...
I0918 14:18:53.046625   46752 kubeadm.go:317] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0918 14:18:53.046625   46752 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0918 14:18:53.047619   46752 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0918 14:18:53.047619   46752 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0918 14:18:53.047619   46752 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0918 14:18:53.048618   46752 kubeadm.go:317] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0918 14:18:53.048618   46752 kubeadm.go:317] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0918 14:18:53.048618   46752 kubeadm.go:317] [addons] Applied essential addon: CoreDNS
I0918 14:18:53.048618   46752 kubeadm.go:317] [addons] Applied essential addon: kube-proxy
I0918 14:18:53.048618   46752 kubeadm.go:317] 
I0918 14:18:53.049619   46752 kubeadm.go:317] Your Kubernetes control-plane has initialized successfully!
I0918 14:18:53.049619   46752 kubeadm.go:317] 
I0918 14:18:53.049619   46752 kubeadm.go:317] To start using your cluster, you need to run the following as a regular user:
I0918 14:18:53.049619   46752 kubeadm.go:317] 
I0918 14:18:53.049619   46752 kubeadm.go:317]   mkdir -p $HOME/.kube
I0918 14:18:53.049619   46752 kubeadm.go:317]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0918 14:18:53.049619   46752 kubeadm.go:317]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0918 14:18:53.049619   46752 kubeadm.go:317] 
I0918 14:18:53.049619   46752 kubeadm.go:317] Alternatively, if you are the root user, you can run:
I0918 14:18:53.049619   46752 kubeadm.go:317] 
I0918 14:18:53.050611   46752 kubeadm.go:317]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0918 14:18:53.050611   46752 kubeadm.go:317] 
I0918 14:18:53.050611   46752 kubeadm.go:317] You should now deploy a pod network to the cluster.
I0918 14:18:53.050611   46752 kubeadm.go:317] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0918 14:18:53.050611   46752 kubeadm.go:317]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0918 14:18:53.050611   46752 kubeadm.go:317] 
I0918 14:18:53.050611   46752 kubeadm.go:317] You can now join any number of control-plane nodes by copying certificate authorities
I0918 14:18:53.051612   46752 kubeadm.go:317] and service account keys on each node and then running the following as root:
I0918 14:18:53.051612   46752 kubeadm.go:317] 
I0918 14:18:53.051612   46752 kubeadm.go:317]   kubeadm join control-plane.minikube.internal:8443 --token vznqg4.re0jqsffszs6m015 \
I0918 14:18:53.051612   46752 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:515881b423429ddf2dc2379cd7aa11033146f197c2fecc9ca70e4749db68c195 \
I0918 14:18:53.051612   46752 kubeadm.go:317] 	--control-plane 
I0918 14:18:53.051612   46752 kubeadm.go:317] 
I0918 14:18:53.052620   46752 kubeadm.go:317] Then you can join any number of worker nodes by running the following on each as root:
I0918 14:18:53.052620   46752 kubeadm.go:317] 
I0918 14:18:53.052620   46752 kubeadm.go:317] kubeadm join control-plane.minikube.internal:8443 --token vznqg4.re0jqsffszs6m015 \
I0918 14:18:53.052620   46752 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:515881b423429ddf2dc2379cd7aa11033146f197c2fecc9ca70e4749db68c195 
I0918 14:18:53.053615   46752 cni.go:95] Creating CNI manager for ""
I0918 14:18:53.053615   46752 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0918 14:18:53.053615   46752 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0918 14:18:53.085499   46752 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0918 14:18:53.085499   46752 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.0/kubectl label nodes minikube.k8s.io/version=v1.27.0 minikube.k8s.io/commit=4243041b7a72319b9be7842a7d34b6767bbdac2b minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2022_09_18T14_18_53_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0918 14:18:53.091503   46752 ops.go:34] apiserver oom_adj: -16
I0918 14:18:53.727779   46752 kubeadm.go:1067] duration metric: took 673.1637ms to wait for elevateKubeSystemPrivileges.
I0918 14:18:53.828717   46752 kubeadm.go:398] StartCluster complete in 28.3514317s
I0918 14:18:53.828717   46752 settings.go:142] acquiring lock: {Name:mkfd19d0c39e98ea1b945a22991fe4b83f30ca37 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:18:53.829270   46752 settings.go:150] Updating kubeconfig:  C:\Users\Hina\.kube\config
I0918 14:18:53.830990   46752 lock.go:35] WriteFile acquiring C:\Users\Hina\.kube\config: {Name:mkad62249077665a6e74f7ccf0ed37ef0bc8f1b6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 14:18:54.432231   46752 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0918 14:18:54.432231   46752 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0918 14:18:54.432231   46752 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0918 14:18:54.432231   46752 addons.go:412] enableAddons start: toEnable=map[], additional=[]
I0918 14:18:54.435768   46752 out.go:177] * Verifying Kubernetes components...
I0918 14:18:54.432231   46752 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0918 14:18:54.436756   46752 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W0918 14:18:54.436756   46752 addons.go:162] addon storage-provisioner should already be in state true
I0918 14:18:54.432231   46752 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0918 14:18:54.438713   46752 host.go:66] Checking if "minikube" exists ...
I0918 14:18:54.438739   46752 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0918 14:18:54.434761   46752 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I0918 14:18:54.481274   46752 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 14:18:54.484850   46752 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0918 14:18:54.488002   46752 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 14:18:54.832156   46752 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0918 14:18:54.847158   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0918 14:18:54.894218   46752 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0918 14:18:54.899345   46752 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0918 14:18:54.899345   46752 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0918 14:18:54.914320   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:54.990832   46752 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0918 14:18:54.990832   46752 addons.go:162] addon default-storageclass should already be in state true
I0918 14:18:54.991413   46752 host.go:66] Checking if "minikube" exists ...
I0918 14:18:55.018547   46752 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 14:18:55.410063   46752 api_server.go:51] waiting for apiserver process to appear ...
I0918 14:18:55.434945   46752 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 14:18:55.457372   46752 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61022 SSHKeyPath:C:\Users\Hina\.minikube\machines\minikube\id_rsa Username:docker}
I0918 14:18:55.606671   46752 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I0918 14:18:55.606671   46752 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0918 14:18:55.629534   46752 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 14:18:55.962657   46752 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61022 SSHKeyPath:C:\Users\Hina\.minikube\machines\minikube\id_rsa Username:docker}
I0918 14:18:56.081739   46752 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0918 14:18:56.279810   46752 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0918 14:18:58.240866   46752 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.25.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (3.4087099s)
I0918 14:18:58.241414   46752 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.8059214s)
I0918 14:18:58.241414   46752 start.go:810] {"host.minikube.internal": 192.168.65.2} host record injected into CoreDNS
I0918 14:18:58.241414   46752 api_server.go:71] duration metric: took 3.8091837s to wait for apiserver process to appear ...
I0918 14:18:58.241414   46752 api_server.go:87] waiting for apiserver healthz status ...
I0918 14:18:58.242681   46752 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:61026/healthz ...
I0918 14:18:58.263433   46752 api_server.go:266] https://127.0.0.1:61026/healthz returned 200:
ok
I0918 14:18:58.268444   46752 api_server.go:140] control plane version: v1.25.0
I0918 14:18:58.268444   46752 api_server.go:130] duration metric: took 27.0298ms to wait for apiserver health ...
I0918 14:18:58.285464   46752 system_pods.go:43] waiting for kube-system pods to appear ...
I0918 14:18:58.357584   46752 system_pods.go:59] 4 kube-system pods found
I0918 14:18:58.357584   46752 system_pods.go:61] "etcd-minikube" [1cf284e1-7785-4643-9c5e-5b05ed8235c1] Running
I0918 14:18:58.357584   46752 system_pods.go:61] "kube-apiserver-minikube" [26ec66b3-57fd-4f12-8def-3028dc9c9f98] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0918 14:18:58.357584   46752 system_pods.go:61] "kube-controller-manager-minikube" [7e84ab82-0588-47cf-bd97-04b7874756d6] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0918 14:18:58.357584   46752 system_pods.go:61] "kube-scheduler-minikube" [75d87d58-fd8c-423e-b17d-10447d3b0053] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0918 14:18:58.357584   46752 system_pods.go:74] duration metric: took 72.1196ms to wait for pod list to return data ...
I0918 14:18:58.357584   46752 kubeadm.go:573] duration metric: took 3.9253531s to wait for : map[apiserver:true system_pods:true] ...
I0918 14:18:58.357584   46752 node_conditions.go:102] verifying NodePressure condition ...
I0918 14:18:58.368397   46752 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0918 14:18:58.368933   46752 node_conditions.go:123] node cpu capacity is 4
I0918 14:18:58.368933   46752 node_conditions.go:105] duration metric: took 11.3487ms to run NodePressure ...
I0918 14:18:58.368933   46752 start.go:216] waiting for startup goroutines ...
I0918 14:18:58.415240   46752 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.3335011s)
I0918 14:18:58.415240   46752 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.1354305s)
I0918 14:18:58.419236   46752 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0918 14:18:58.420235   46752 addons.go:414] enableAddons completed in 3.9880039s
I0918 14:18:58.623680   46752 start.go:506] kubectl: 1.24.2, cluster: 1.25.0 (minor skew: 1)
I0918 14:18:58.625426   46752 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Sun 2022-09-18 18:18:06 UTC, end at Sun 2022-09-18 19:52:15 UTC. --
Sep 18 18:18:19 minikube dockerd[504]: time="2022-09-18T18:18:19.602479100Z" level=info msg="Daemon shutdown complete"
Sep 18 18:18:19 minikube dockerd[504]: time="2022-09-18T18:18:19.602545600Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Sep 18 18:18:19 minikube systemd[1]: docker.service: Succeeded.
Sep 18 18:18:19 minikube systemd[1]: Stopped Docker Application Container Engine.
Sep 18 18:18:19 minikube systemd[1]: Starting Docker Application Container Engine...
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.671436500Z" level=info msg="Starting up"
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.674941700Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.675004300Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.675036300Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.675053600Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.677594900Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.677877900Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.677990000Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.678012900Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.700345200Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.711648600Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.711711300Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.711724700Z" level=warning msg="Your kernel does not support cgroup blkio throttle.read_bps_device"
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.711732800Z" level=warning msg="Your kernel does not support cgroup blkio throttle.write_bps_device"
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.711740300Z" level=warning msg="Your kernel does not support cgroup blkio throttle.read_iops_device"
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.711747300Z" level=warning msg="Your kernel does not support cgroup blkio throttle.write_iops_device"
Sep 18 18:18:19 minikube dockerd[711]: time="2022-09-18T18:18:19.712092400Z" level=info msg="Loading containers: start."
Sep 18 18:18:21 minikube dockerd[711]: time="2022-09-18T18:18:21.309081500Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 18 18:18:21 minikube dockerd[711]: time="2022-09-18T18:18:21.980379300Z" level=info msg="Loading containers: done."
Sep 18 18:18:22 minikube dockerd[711]: time="2022-09-18T18:18:22.011499900Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Sep 18 18:18:22 minikube dockerd[711]: time="2022-09-18T18:18:22.011650000Z" level=info msg="Daemon has completed initialization"
Sep 18 18:18:22 minikube systemd[1]: Started Docker Application Container Engine.
Sep 18 18:18:22 minikube dockerd[711]: time="2022-09-18T18:18:22.064176700Z" level=info msg="API listen on [::]:2376"
Sep 18 18:18:22 minikube dockerd[711]: time="2022-09-18T18:18:22.071632100Z" level=info msg="API listen on /var/run/docker.sock"
Sep 18 18:19:28 minikube dockerd[711]: time="2022-09-18T18:19:28.696926400Z" level=info msg="ignoring event" container=c74a85f27d660442503220f6a3f7f0273f2a6f3c12ff5db53ebdf7e573339660 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:29:58 minikube dockerd[711]: time="2022-09-18T18:29:58.406263000Z" level=info msg="Layer sha256:fa9e1b278cb14dd94bb01f96cd30989b596e7b5c84014fbd114fc478ce007f8b cleaned up"
Sep 18 18:29:59 minikube dockerd[711]: time="2022-09-18T18:29:59.635090800Z" level=info msg="Layer sha256:58d10a14031085e7beb4456e4223a4d80ccb7fbdee11b03e7fad62b98ee35749 cleaned up"
Sep 18 18:38:08 minikube dockerd[711]: time="2022-09-18T18:38:08.081303800Z" level=info msg="ignoring event" container=895959124bd49d561567a8edf8192c7023fa49033637ee6282dc79332124f065 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:38:16 minikube dockerd[711]: time="2022-09-18T18:38:16.936367800Z" level=info msg="ignoring event" container=427820df9d0e1218e3939c8505d0955fd12ccc0013c4b96a0c8a63e257d29d10 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:38:38 minikube dockerd[711]: time="2022-09-18T18:38:38.300134000Z" level=info msg="ignoring event" container=b2ac3d88cea4525fa864165618df235994d550c4f7406ccea202c11344a69db3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:39:08 minikube dockerd[711]: time="2022-09-18T18:39:08.760565400Z" level=info msg="ignoring event" container=040ef7a34a24ed08420567645aa7508a06a02783fab0cb554b73f011f43e2d4e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:40:08 minikube dockerd[711]: time="2022-09-18T18:40:08.712501500Z" level=info msg="ignoring event" container=8b92783880f91dfb4629cf584b1f1bd83d3a8362959877e80717cfe5987136be module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:41:37 minikube dockerd[711]: time="2022-09-18T18:41:37.488450700Z" level=info msg="ignoring event" container=6be805f6084e8b72db6b1488aaf1b65e4bcaa07f76f1020d317273e6bded2457 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:44:33 minikube dockerd[711]: time="2022-09-18T18:44:33.654663300Z" level=info msg="ignoring event" container=32aaa62cea8b57aa2b936b44ff61185e942398bb61445badd1148f0117dfb003 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:49:26 minikube dockerd[711]: time="2022-09-18T18:49:26.177260700Z" level=info msg="Layer sha256:79f55fa221ad94193f9fc2821e75c535d87423a5791ca4d52763b4a4b6af9efd cleaned up"
Sep 18 18:49:45 minikube dockerd[711]: time="2022-09-18T18:49:45.578590800Z" level=info msg="ignoring event" container=aad1b9b8a0daee513c15cf688a2499dde7031a1ac78ce37ec1fc510c92e09aa4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:50:22 minikube dockerd[711]: time="2022-09-18T18:50:22.273050700Z" level=info msg="ignoring event" container=4e441d0b0ca62a3c9ea0c987dcbfc0a4415a8085aeed71fe60d521a4e030d991 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:50:36 minikube dockerd[711]: time="2022-09-18T18:50:36.220989800Z" level=info msg="ignoring event" container=62ce293bbf7a1e016b3c73bfe7532a6ef432a2c0019949d3923f4ce0e35427de module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:50:42 minikube dockerd[711]: time="2022-09-18T18:50:42.553957600Z" level=info msg="ignoring event" container=eb7bdbee830a5cc2d74f4d29b19332e5b78fd5db223ee11f7ce05ff75e780784 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:51:04 minikube dockerd[711]: time="2022-09-18T18:51:04.240906200Z" level=info msg="ignoring event" container=27a7b7dac1e4950c5588794af12ed7edf163711c75318eef9d375024d767fe16 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:51:37 minikube dockerd[711]: time="2022-09-18T18:51:37.436566800Z" level=info msg="ignoring event" container=4401cb9a91b5c242ad2c3d40f5fcfd60b6a668da942265badedfc2989aa0e575 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:52:36 minikube dockerd[711]: time="2022-09-18T18:52:36.305465700Z" level=info msg="ignoring event" container=2db729032e471025a6b038699d54f1683b637d313b0a30861ed8bef2b7f3c0c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:54:15 minikube dockerd[711]: time="2022-09-18T18:54:15.005824500Z" level=info msg="ignoring event" container=2cba22650de956658a01c78c64ea712f83ae5587c1b728116ad1da80184df034 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 18:57:14 minikube dockerd[711]: time="2022-09-18T18:57:14.141106700Z" level=info msg="ignoring event" container=cafaf7e88eab72c40d5c3be1371032d807a91ea337d0995f3026765c014b4874 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:02:30 minikube dockerd[711]: time="2022-09-18T19:02:30.943427000Z" level=info msg="ignoring event" container=1eaa052c8f64d172d0d55980f99eff331479c84db0e5df7c3f474f9d2a195505 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:07:51 minikube dockerd[711]: time="2022-09-18T19:07:51.869784800Z" level=info msg="ignoring event" container=38428f64dccf1577b8b6846f6481bd82eeb4c5a7a00873d1f0834402f32db1ab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:13:10 minikube dockerd[711]: time="2022-09-18T19:13:10.744876100Z" level=info msg="ignoring event" container=c722fc42fbde257bc0d40ed931ae74534a2cc2be42c2453121b52357d8512a31 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:18:17 minikube dockerd[711]: time="2022-09-18T19:18:17.892896600Z" level=info msg="ignoring event" container=ceb9eeac5006d0b38b71e3970fcf81ff220f4de1ad3824e7311199a89116bf07 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:23:52 minikube dockerd[711]: time="2022-09-18T19:23:52.276950900Z" level=info msg="ignoring event" container=ef83f84391498e7b95acf8c64538ea0c8bdbdc66f3554b2e154700ca93e20245 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:29:06 minikube dockerd[711]: time="2022-09-18T19:29:06.967831100Z" level=info msg="ignoring event" container=8caad2a1d6c145b1392cb0950d38974b45fd3a7114674e469e0c5f1c6e426f25 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:34:36 minikube dockerd[711]: time="2022-09-18T19:34:36.723436800Z" level=info msg="ignoring event" container=956026652d8bfa0803d546b671f831064eed2a0c111dbd7129850d2cea9715b9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:36:03 minikube dockerd[711]: time="2022-09-18T19:36:03.253298200Z" level=info msg="ignoring event" container=4c353a1beed7296a1078408a4a563f8bf2d6240245db7276f718f2498c6b494c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 19:36:36 minikube dockerd[711]: time="2022-09-18T19:36:36.302397600Z" level=info msg="Layer sha256:c9aebcf8f77caa42ccc27edaa4d6618f8373b794d6583e30a0875fe5e7be8a01 cleaned up"
Sep 18 19:46:54 minikube dockerd[711]: time="2022-09-18T19:46:54.130609300Z" level=warning msg="reference for unknown type: " digest="sha256:4af9580485920635d888efe1eddbd67e12f9d5d84dba87100e93feb4e46636b3" remote="docker.io/kubernetesui/dashboard@sha256:4af9580485920635d888efe1eddbd67e12f9d5d84dba87100e93feb4e46636b3"
Sep 18 19:48:45 minikube dockerd[711]: time="2022-09-18T19:48:45.596924100Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID
8ce871e53ade9       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   3 minutes ago       Running             dashboard-metrics-scraper   0                   9f2237d7bfb7e
db4f909be2f62       kubernetesui/dashboard@sha256:4af9580485920635d888efe1eddbd67e12f9d5d84dba87100e93feb4e46636b3         3 minutes ago       Running             kubernetes-dashboard        0                   bf0b4efb96ea3
6b853b4718ecf       62f24413b6683                                                                                          13 minutes ago      Running             springboot-k8s-demo         0                   d8b1865de23be
a0cded536fdb7       6e38f40d628db                                                                                          2 hours ago         Running             storage-provisioner         1                   43b1c7168e979
c74a85f27d660       6e38f40d628db                                                                                          2 hours ago         Exited              storage-provisioner         0                   43b1c7168e979
a77015314be2e       5185b96f0becf                                                                                          2 hours ago         Running             coredns                     0                   38d095a5522fd
182f7b397fe9d       58a9a0c6d96f2                                                                                          2 hours ago         Running             kube-proxy                  0                   0d3d9512027a9
aa89259e717f4       1a54c86c03a67                                                                                          2 hours ago         Running             kube-controller-manager     0                   da861696ec77d
2c196ab4ac6b9       4d2edfd10d3e3                                                                                          2 hours ago         Running             kube-apiserver              0                   b5f81be4ee76f
5e524cf110cf8       bef2cf3115095                                                                                          2 hours ago         Running             kube-scheduler              0                   45ec150f99528
ce8236ef3b647       a8a176a5d5d69                                                                                          2 hours ago         Running             etcd                        0                   57cafd8482fdb

* 
* ==> coredns [a77015314be2] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = a1b5920ef1e8e10875eeec3214b810e7e404fdaf6cfe53f31cc42ae1e9ba5884ecf886330489b6b02fba5b37a31406fcb402b2501c7ab0318fc890d74b6fae55
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: connection refused

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=4243041b7a72319b9be7842a7d34b6767bbdac2b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_09_18T14_18_53_0700
                    minikube.k8s.io/version=v1.27.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 18 Sep 2022 18:18:48 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 18 Sep 2022 19:52:09 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 18 Sep 2022 19:49:47 +0000   Sun, 18 Sep 2022 18:18:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 18 Sep 2022 19:49:47 +0000   Sun, 18 Sep 2022 18:18:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 18 Sep 2022 19:49:47 +0000   Sun, 18 Sep 2022 18:18:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 18 Sep 2022 19:49:47 +0000   Sun, 18 Sep 2022 18:19:03 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             9717048Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             9717048Ki
  pods:               110
System Info:
  Machine ID:                 40026c506a9948afaae3b0fda0a61c83
  System UUID:                40026c506a9948afaae3b0fda0a61c83
  Boot ID:                    f9d80c11-8f05-41e5-8877-3e55a5ae3ab4
  Kernel Version:             5.10.16.3-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.25.0
  Kube-Proxy Version:         v1.25.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     spring-boot-k8s-7c9884547b-xfvm9             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13m
  kube-system                 coredns-565d847f94-47slz                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     93m
  kube-system                 etcd-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         93m
  kube-system                 kube-apiserver-minikube                      250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93m
  kube-system                 kube-controller-manager-minikube             200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93m
  kube-system                 kube-proxy-hn5s9                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93m
  kube-system                 kube-scheduler-minikube                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93m
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93m
  kubernetes-dashboard        dashboard-metrics-scraper-b74747df5-m9rj4    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m25s
  kubernetes-dashboard        kubernetes-dashboard-54596f475f-g85tw        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m25s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (1%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Sep18 17:55] WSL2: Performing memory compaction.
[Sep18 17:58] WSL2: Performing memory compaction.
[Sep18 18:02] WSL2: Performing memory compaction.
[Sep18 18:03] WSL2: Performing memory compaction.
[Sep18 18:04] WSL2: Performing memory compaction.
[Sep18 18:06] WSL2: Performing memory compaction.
[Sep18 18:07] WSL2: Performing memory compaction.
[Sep18 18:09] WSL2: Performing memory compaction.
[Sep18 18:11] WSL2: Performing memory compaction.
[Sep18 18:12] WSL2: Performing memory compaction.
[Sep18 18:13] WSL2: Performing memory compaction.
[Sep18 18:14] WSL2: Performing memory compaction.
[Sep18 18:15] WSL2: Performing memory compaction.
[Sep18 18:16] WSL2: Performing memory compaction.
[Sep18 18:18] WSL2: Performing memory compaction.
[Sep18 18:20] WSL2: Performing memory compaction.
[Sep18 18:21] WSL2: Performing memory compaction.
[Sep18 18:23] WSL2: Performing memory compaction.
[Sep18 18:28] WSL2: Performing memory compaction.
[Sep18 18:32] WSL2: Performing memory compaction.
[Sep18 18:34] WSL2: Performing memory compaction.
[Sep18 18:35] WSL2: Performing memory compaction.
[Sep18 18:36] WSL2: Performing memory compaction.
[Sep18 18:38] WSL2: Performing memory compaction.
[Sep18 18:40] WSL2: Performing memory compaction.
[Sep18 18:41] WSL2: Performing memory compaction.
[Sep18 18:44] WSL2: Performing memory compaction.
[Sep18 18:46] WSL2: Performing memory compaction.
[Sep18 18:47] WSL2: Performing memory compaction.
[Sep18 18:49] WSL2: Performing memory compaction.
[Sep18 18:52] WSL2: Performing memory compaction.
[Sep18 18:53] WSL2: Performing memory compaction.
[Sep18 18:55] WSL2: Performing memory compaction.
[Sep18 18:57] WSL2: Performing memory compaction.
[Sep18 19:00] WSL2: Performing memory compaction.
[Sep18 19:02] WSL2: Performing memory compaction.
[Sep18 19:05] WSL2: Performing memory compaction.
[Sep18 19:07] WSL2: Performing memory compaction.
[Sep18 19:08] WSL2: Performing memory compaction.
[Sep18 19:10] WSL2: Performing memory compaction.
[Sep18 19:15] WSL2: Performing memory compaction.
[Sep18 19:16] WSL2: Performing memory compaction.
[Sep18 19:17] WSL2: Performing memory compaction.
[Sep18 19:18] WSL2: Performing memory compaction.
[Sep18 19:23] WSL2: Performing memory compaction.
[Sep18 19:24] WSL2: Performing memory compaction.
[Sep18 19:26] WSL2: Performing memory compaction.
[Sep18 19:28] WSL2: Performing memory compaction.
[Sep18 19:30] WSL2: Performing memory compaction.
[Sep18 19:33] WSL2: Performing memory compaction.
[Sep18 19:35] WSL2: Performing memory compaction.
[Sep18 19:36] WSL2: Performing memory compaction.
[Sep18 19:37] WSL2: Performing memory compaction.
[Sep18 19:39] WSL2: Performing memory compaction.
[Sep18 19:42] WSL2: Performing memory compaction.
[Sep18 19:43] WSL2: Performing memory compaction.
[Sep18 19:45] WSL2: Performing memory compaction.
[Sep18 19:46] WSL2: Performing memory compaction.
[Sep18 19:48] WSL2: Performing memory compaction.
[Sep18 19:51] WSL2: Performing memory compaction.

* 
* ==> etcd [ce8236ef3b64] <==
* {"level":"warn","ts":"2022-09-18T19:37:12.109Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128015814950812401,"retry-timeout":"500ms"}
{"level":"info","ts":"2022-09-18T19:37:12.160Z","caller":"traceutil/trace.go:171","msg":"trace[212741841] linearizableReadLoop","detail":"{readStateIndex:4882; appliedIndex:4882; }","duration":"552.0557ms","start":"2022-09-18T19:37:11.608Z","end":"2022-09-18T19:37:12.160Z","steps":["trace[212741841] 'read index received'  (duration: 552.0046ms)","trace[212741841] 'applied index is now lower than readState.Index'  (duration: 48.8Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:37:12.164Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"555.8529ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:37:12.164Z","caller":"traceutil/trace.go:171","msg":"trace[1483716866] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3893; }","duration":"555.9726ms","start":"2022-09-18T19:37:11.608Z","end":"2022-09-18T19:37:12.164Z","steps":["trace[1483716866] 'agreement among raft nodes before linearized reading'  (duration: 552.212ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-18T19:37:12.164Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-18T19:37:11.608Z","time spent":"556.0477ms","remote":"127.0.0.1:60348","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2022-09-18T19:38:51.728Z","caller":"traceutil/trace.go:171","msg":"trace[1133258803] linearizableReadLoop","detail":"{readStateIndex:4972; appliedIndex:4971; }","duration":"118.2936ms","start":"2022-09-18T19:38:51.610Z","end":"2022-09-18T19:38:51.728Z","steps":["trace[1133258803] 'read index received'  (duration: 64.1774ms)","trace[1133258803] 'applied index is now lower than readState.Index'  (duration: 54.1141ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:38:51.728Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"118.5225ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:38:51.728Z","caller":"traceutil/trace.go:171","msg":"trace[540723436] compact","detail":"{revision:3737; response_revision:3962; }","duration":"152.7315ms","start":"2022-09-18T19:38:51.575Z","end":"2022-09-18T19:38:51.728Z","steps":["trace[540723436] 'process raft request'  (duration: 98.4408ms)","trace[540723436] 'check and update compact revision'  (duration: 53.9413ms)"],"step_count":2}
{"level":"info","ts":"2022-09-18T19:38:51.728Z","caller":"traceutil/trace.go:171","msg":"trace[466220950] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3962; }","duration":"118.5988ms","start":"2022-09-18T19:38:51.609Z","end":"2022-09-18T19:38:51.728Z","steps":["trace[466220950] 'agreement among raft nodes before linearized reading'  (duration: 118.4931ms)"],"step_count":1}
{"level":"info","ts":"2022-09-18T19:38:51.729Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3737}
{"level":"info","ts":"2022-09-18T19:38:51.730Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":3737,"took":"654.6Âµs"}
{"level":"info","ts":"2022-09-18T19:43:51.743Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3962}
{"level":"info","ts":"2022-09-18T19:43:51.746Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":3962,"took":"2.4772ms"}
{"level":"warn","ts":"2022-09-18T19:44:32.037Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"119.0234ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:44:32.037Z","caller":"traceutil/trace.go:171","msg":"trace[97726355] range","detail":"{range_begin:/registry/persistentvolumes/; range_end:/registry/persistentvolumes0; response_count:0; response_revision:4228; }","duration":"119.2195ms","start":"2022-09-18T19:44:31.918Z","end":"2022-09-18T19:44:32.037Z","steps":["trace[97726355] 'count revisions from in-memory index tree'  (duration: 118.8939ms)"],"step_count":1}
{"level":"info","ts":"2022-09-18T19:46:53.847Z","caller":"traceutil/trace.go:171","msg":"trace[693096669] linearizableReadLoop","detail":"{readStateIndex:5501; appliedIndex:5501; }","duration":"196.8208ms","start":"2022-09-18T19:46:53.650Z","end":"2022-09-18T19:46:53.847Z","steps":["trace[693096669] 'read index received'  (duration: 196.8135ms)","trace[693096669] 'applied index is now lower than readState.Index'  (duration: 6.3Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:46:53.852Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"138.281ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:698"}
{"level":"info","ts":"2022-09-18T19:46:53.852Z","caller":"traceutil/trace.go:171","msg":"trace[1569793731] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:4388; }","duration":"138.4036ms","start":"2022-09-18T19:46:53.714Z","end":"2022-09-18T19:46:53.852Z","steps":["trace[1569793731] 'agreement among raft nodes before linearized reading'  (duration: 133.4367ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-18T19:46:53.853Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"202.8836ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:46:53.853Z","caller":"traceutil/trace.go:171","msg":"trace[539778819] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4388; }","duration":"202.957ms","start":"2022-09-18T19:46:53.650Z","end":"2022-09-18T19:46:53.853Z","steps":["trace[539778819] 'agreement among raft nodes before linearized reading'  (duration: 196.9798ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-18T19:47:01.412Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"239.8772ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:47:01.412Z","caller":"traceutil/trace.go:171","msg":"trace[11950353] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4394; }","duration":"239.9989ms","start":"2022-09-18T19:47:01.172Z","end":"2022-09-18T19:47:01.412Z","steps":["trace[11950353] 'range keys from in-memory index tree'  (duration: 239.8548ms)"],"step_count":1}
{"level":"info","ts":"2022-09-18T19:47:03.287Z","caller":"traceutil/trace.go:171","msg":"trace[1464797335] linearizableReadLoop","detail":"{readStateIndex:5511; appliedIndex:5511; }","duration":"173.3323ms","start":"2022-09-18T19:47:03.114Z","end":"2022-09-18T19:47:03.287Z","steps":["trace[1464797335] 'read index received'  (duration: 173.3199ms)","trace[1464797335] 'applied index is now lower than readState.Index'  (duration: 10.2Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:47:03.300Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"186.1033ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:47:03.300Z","caller":"traceutil/trace.go:171","msg":"trace[2012508691] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:4396; }","duration":"186.3205ms","start":"2022-09-18T19:47:03.114Z","end":"2022-09-18T19:47:03.300Z","steps":["trace[2012508691] 'agreement among raft nodes before linearized reading'  (duration: 173.6508ms)"],"step_count":1}
{"level":"info","ts":"2022-09-18T19:47:18.900Z","caller":"traceutil/trace.go:171","msg":"trace[38850347] transaction","detail":"{read_only:false; response_revision:4407; number_of_response:1; }","duration":"233.5308ms","start":"2022-09-18T19:47:18.667Z","end":"2022-09-18T19:47:18.900Z","steps":["trace[38850347] 'process raft request'  (duration: 204.9797ms)","trace[38850347] 'compare'  (duration: 28.3974ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:47:18.900Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"180.7897ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:47:18.901Z","caller":"traceutil/trace.go:171","msg":"trace[1457575308] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4407; }","duration":"180.8634ms","start":"2022-09-18T19:47:18.720Z","end":"2022-09-18T19:47:18.901Z","steps":["trace[1457575308] 'agreement among raft nodes before linearized reading'  (duration: 180.7155ms)"],"step_count":1}
{"level":"info","ts":"2022-09-18T19:47:18.900Z","caller":"traceutil/trace.go:171","msg":"trace[38826885] linearizableReadLoop","detail":"{readStateIndex:5526; appliedIndex:5525; }","duration":"180.5613ms","start":"2022-09-18T19:47:18.720Z","end":"2022-09-18T19:47:18.900Z","steps":["trace[38826885] 'read index received'  (duration: 151.9176ms)","trace[38826885] 'applied index is now lower than readState.Index'  (duration: 28.6418ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:48:18.164Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128015814950815830,"retry-timeout":"500ms"}
{"level":"info","ts":"2022-09-18T19:48:18.479Z","caller":"traceutil/trace.go:171","msg":"trace[1182709094] linearizableReadLoop","detail":"{readStateIndex:5579; appliedIndex:5579; }","duration":"815.4681ms","start":"2022-09-18T19:48:17.663Z","end":"2022-09-18T19:48:18.479Z","steps":["trace[1182709094] 'read index received'  (duration: 815.4587ms)","trace[1182709094] 'applied index is now lower than readState.Index'  (duration: 7.8Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:48:18.495Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"832.127ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:48:18.495Z","caller":"traceutil/trace.go:171","msg":"trace[1814256219] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4448; }","duration":"832.2796ms","start":"2022-09-18T19:48:17.663Z","end":"2022-09-18T19:48:18.495Z","steps":["trace[1814256219] 'agreement among raft nodes before linearized reading'  (duration: 815.8202ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-18T19:48:18.496Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-18T19:48:17.663Z","time spent":"832.3805ms","remote":"127.0.0.1:60348","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-09-18T19:48:38.152Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128015814950815926,"retry-timeout":"500ms"}
{"level":"info","ts":"2022-09-18T19:48:38.543Z","caller":"traceutil/trace.go:171","msg":"trace[1422979915] linearizableReadLoop","detail":"{readStateIndex:5597; appliedIndex:5597; }","duration":"892.411ms","start":"2022-09-18T19:48:37.651Z","end":"2022-09-18T19:48:38.543Z","steps":["trace[1422979915] 'read index received'  (duration: 892.4006ms)","trace[1422979915] 'applied index is now lower than readState.Index'  (duration: 8.8Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:48:38.546Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"895.4228ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:48:38.547Z","caller":"traceutil/trace.go:171","msg":"trace[304223671] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4462; }","duration":"895.558ms","start":"2022-09-18T19:48:37.651Z","end":"2022-09-18T19:48:38.546Z","steps":["trace[304223671] 'agreement among raft nodes before linearized reading'  (duration: 892.611ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-18T19:48:38.547Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-18T19:48:37.651Z","time spent":"895.6898ms","remote":"127.0.0.1:60348","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-09-18T19:48:39.535Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"849.9781ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015814950815935 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:4456 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128015814950815933 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2022-09-18T19:48:39.535Z","caller":"traceutil/trace.go:171","msg":"trace[2098790118] linearizableReadLoop","detail":"{readStateIndex:5599; appliedIndex:5598; }","duration":"874.3967ms","start":"2022-09-18T19:48:38.661Z","end":"2022-09-18T19:48:39.535Z","steps":["trace[2098790118] 'read index received'  (duration: 24.1349ms)","trace[2098790118] 'applied index is now lower than readState.Index'  (duration: 850.2598ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:48:39.535Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"874.5499ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:48:39.535Z","caller":"traceutil/trace.go:171","msg":"trace[1915154238] transaction","detail":"{read_only:false; response_revision:4463; number_of_response:1; }","duration":"893.1418ms","start":"2022-09-18T19:48:38.642Z","end":"2022-09-18T19:48:39.535Z","steps":["trace[1915154238] 'process raft request'  (duration: 42.8788ms)","trace[1915154238] 'compare'  (duration: 849.6041ms)"],"step_count":2}
{"level":"info","ts":"2022-09-18T19:48:39.535Z","caller":"traceutil/trace.go:171","msg":"trace[1606936691] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4463; }","duration":"874.5994ms","start":"2022-09-18T19:48:38.661Z","end":"2022-09-18T19:48:39.535Z","steps":["trace[1606936691] 'agreement among raft nodes before linearized reading'  (duration: 874.481ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-18T19:48:39.535Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-18T19:48:38.661Z","time spent":"874.6584ms","remote":"127.0.0.1:60348","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-09-18T19:48:39.535Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-18T19:48:38.642Z","time spent":"893.2802ms","remote":"127.0.0.1:60248","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:4456 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128015814950815933 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2022-09-18T19:48:41.918Z","caller":"traceutil/trace.go:171","msg":"trace[141773216] linearizableReadLoop","detail":"{readStateIndex:5601; appliedIndex:5601; }","duration":"176.1245ms","start":"2022-09-18T19:48:41.741Z","end":"2022-09-18T19:48:41.917Z","steps":["trace[141773216] 'read index received'  (duration: 176.0825ms)","trace[141773216] 'applied index is now lower than readState.Index'  (duration: 39.4Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:48:42.218Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"476.1444ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2022-09-18T19:48:42.218Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"298.5294ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:48:42.218Z","caller":"traceutil/trace.go:171","msg":"trace[18056965] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4465; }","duration":"476.2839ms","start":"2022-09-18T19:48:41.741Z","end":"2022-09-18T19:48:42.218Z","steps":["trace[18056965] 'agreement among raft nodes before linearized reading'  (duration: 176.3613ms)","trace[18056965] 'range keys from in-memory index tree'  (duration: 299.7834ms)"],"step_count":2}
{"level":"info","ts":"2022-09-18T19:48:42.218Z","caller":"traceutil/trace.go:171","msg":"trace[463619576] range","detail":"{range_begin:/registry/persistentvolumeclaims/; range_end:/registry/persistentvolumeclaims0; response_count:0; response_revision:4465; }","duration":"298.5755ms","start":"2022-09-18T19:48:41.919Z","end":"2022-09-18T19:48:42.218Z","steps":["trace[463619576] 'count revisions from in-memory index tree'  (duration: 298.344ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-18T19:48:42.218Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-18T19:48:41.741Z","time spent":"476.3648ms","remote":"127.0.0.1:60348","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-09-18T19:48:42.218Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"298.9644ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/cronjobs/\" range_end:\"/registry/cronjobs0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:48:42.218Z","caller":"traceutil/trace.go:171","msg":"trace[1266307266] range","detail":"{range_begin:/registry/cronjobs/; range_end:/registry/cronjobs0; response_count:0; response_revision:4465; }","duration":"299.3605ms","start":"2022-09-18T19:48:41.919Z","end":"2022-09-18T19:48:42.218Z","steps":["trace[1266307266] 'count revisions from in-memory index tree'  (duration: 298.6782ms)"],"step_count":1}
{"level":"info","ts":"2022-09-18T19:48:51.761Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4200}
{"level":"info","ts":"2022-09-18T19:48:51.764Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":4200,"took":"1.2221ms"}
{"level":"info","ts":"2022-09-18T19:51:35.975Z","caller":"traceutil/trace.go:171","msg":"trace[1477134366] linearizableReadLoop","detail":"{readStateIndex:5781; appliedIndex:5781; }","duration":"306.2112ms","start":"2022-09-18T19:51:35.668Z","end":"2022-09-18T19:51:35.975Z","steps":["trace[1477134366] 'read index received'  (duration: 306.2009ms)","trace[1477134366] 'applied index is now lower than readState.Index'  (duration: 8.3Âµs)"],"step_count":2}
{"level":"warn","ts":"2022-09-18T19:51:35.976Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"307.4581ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-09-18T19:51:35.976Z","caller":"traceutil/trace.go:171","msg":"trace[578447681] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4608; }","duration":"307.5672ms","start":"2022-09-18T19:51:35.668Z","end":"2022-09-18T19:51:35.976Z","steps":["trace[578447681] 'agreement among raft nodes before linearized reading'  (duration: 306.4261ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-18T19:51:35.976Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-18T19:51:35.668Z","time spent":"307.6415ms","remote":"127.0.0.1:60348","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}

* 
* ==> kernel <==
*  19:52:16 up  5:31,  0 users,  load average: 1.01, 0.88, 0.90
Linux minikube 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [2c196ab4ac6b] <==
* {"level":"warn","ts":"2022-09-18T19:24:25.607Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002fe0540/127.0.0.1:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2022-09-18T19:24:27.618Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0021b9180/127.0.0.1:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0918 19:24:29.082371       1 trace.go:205] Trace[167072106]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:7c5b0baf-dff7-4701-87c9-41ac7eff3ffd,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (18-Sep-2022 19:24:22.409) (total time: 6672ms):
Trace[167072106]: ---"About to write a response" 6672ms (19:24:29.082)
Trace[167072106]: [6.6725582s] [6.6725582s] END
I0918 19:24:29.082692       1 trace.go:205] Trace[1052905658]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.25.0 (linux/amd64) kubernetes/a866cbe,audit-id:8f517c78-60cc-4742-8b90-2ce55ad33cfb,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (18-Sep-2022 19:24:26.442) (total time: 2639ms):
Trace[1052905658]: ---"About to write a response" 2639ms (19:24:29.082)
Trace[1052905658]: [2.639706s] [2.639706s] END
I0918 19:24:29.083019       1 trace.go:205] Trace[1317711479]: "GuaranteedUpdate etcd3" audit-id:374d1e54-029d-40c5-bcfd-85a8e0290e9b,key:/leases/kube-node-lease/minikube,type:*coordination.Lease (18-Sep-2022 19:24:23.676) (total time: 5406ms):
Trace[1317711479]: ---"Txn call finished" err:<nil> 5405ms (19:24:29.082)
Trace[1317711479]: [5.4063819s] [5.4063819s] END
I0918 19:24:29.083252       1 trace.go:205] Trace[1573967842]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.25.0 (linux/amd64) kubernetes/a866cbe,audit-id:374d1e54-029d-40c5-bcfd-85a8e0290e9b,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (18-Sep-2022 19:24:23.676) (total time: 5406ms):
Trace[1573967842]: ---"Write to database call finished" len:476,err:<nil> 5406ms (19:24:29.083)
Trace[1573967842]: [5.4069009s] [5.4069009s] END
I0918 19:24:29.818507       1 trace.go:205] Trace[79846965]: "GuaranteedUpdate etcd3" audit-id:595651f4-adef-4d41-ab9d-ead3f148cc33,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints (18-Sep-2022 19:24:29.092) (total time: 726ms):
Trace[79846965]: ---"Txn call finished" err:<nil> 725ms (19:24:29.818)
Trace[79846965]: [726.4188ms] [726.4188ms] END
I0918 19:24:29.818704       1 trace.go:205] Trace[635244858]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:595651f4-adef-4d41-ab9d-ead3f148cc33,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (18-Sep-2022 19:24:29.091) (total time: 727ms):
Trace[635244858]: ---"Write to database call finished" len:735,err:<nil> 726ms (19:24:29.818)
Trace[635244858]: [727.1447ms] [727.1447ms] END
I0918 19:24:29.818937       1 trace.go:205] Trace[2009333804]: "Get" url:/api/v1/namespaces/default/services/kubernetes,user-agent:kube-apiserver/v1.25.0 (linux/amd64) kubernetes/a866cbe,audit-id:2ab362fe-71ce-4a1b-9792-a9c6563751e6,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (18-Sep-2022 19:24:29.083) (total time: 735ms):
Trace[2009333804]: ---"About to write a response" 734ms (19:24:29.818)
Trace[2009333804]: [735.0438ms] [735.0438ms] END
I0918 19:24:29.818507       1 trace.go:205] Trace[51375321]: "GuaranteedUpdate etcd3" audit-id:093aeb74-7153-44e1-a071-eb159c115ece,key:/events/kube-system/kube-apiserver-minikube.1716094284ee8cc8,type:*core.Event (18-Sep-2022 19:24:23.610) (total time: 6207ms):
Trace[51375321]: ---"initial value restored" 5471ms (19:24:29.082)
Trace[51375321]: ---"Txn call finished" err:<nil> 734ms (19:24:29.818)
Trace[51375321]: [6.2078599s] [6.2078599s] END
I0918 19:24:29.819545       1 trace.go:205] Trace[652627103]: "Patch" url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.1716094284ee8cc8,user-agent:kubelet/v1.25.0 (linux/amd64) kubernetes/a866cbe,audit-id:093aeb74-7153-44e1-a071-eb159c115ece,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (18-Sep-2022 19:24:23.610) (total time: 6209ms):
Trace[652627103]: ---"About to apply patch" 5471ms (19:24:29.082)
Trace[652627103]: ---"Object stored in database" 735ms (19:24:29.819)
Trace[652627103]: [6.2090492s] [6.2090492s] END
{"level":"warn","ts":"2022-09-18T19:28:15.634Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002fe0540/127.0.0.1:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0918 19:28:16.223117       1 trace.go:205] Trace[1266031754]: "GuaranteedUpdate etcd3" audit-id:74201231-3fb1-4422-b802-256806a30384,key:/leases/kube-node-lease/minikube,type:*coordination.Lease (18-Sep-2022 19:28:13.751) (total time: 2471ms):
Trace[1266031754]: ---"Txn call finished" err:<nil> 2470ms (19:28:16.223)
Trace[1266031754]: [2.471095s] [2.471095s] END
I0918 19:28:16.223381       1 trace.go:205] Trace[1942098758]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.25.0 (linux/amd64) kubernetes/a866cbe,audit-id:74201231-3fb1-4422-b802-256806a30384,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (18-Sep-2022 19:28:13.751) (total time: 2471ms):
Trace[1942098758]: ---"Write to database call finished" len:476,err:<nil> 2471ms (19:28:16.223)
Trace[1942098758]: [2.4715766s] [2.4715766s] END
I0918 19:28:16.232718       1 trace.go:205] Trace[1389168771]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:bdfc8990-373b-4d3c-abf3-385cd74cf842,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (18-Sep-2022 19:28:15.276) (total time: 956ms):
Trace[1389168771]: ---"About to write a response" 956ms (19:28:16.232)
Trace[1389168771]: [956.2058ms] [956.2058ms] END
I0918 19:29:59.630421       1 trace.go:205] Trace[422276687]: "Get" url:/api/v1/namespaces/kube-system,user-agent:kube-apiserver/v1.25.0 (linux/amd64) kubernetes/a866cbe,audit-id:97b5c7e9-824e-466a-a829-87c1c33f1e95,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (18-Sep-2022 19:29:58.641) (total time: 989ms):
Trace[422276687]: ---"About to write a response" 988ms (19:29:59.630)
Trace[422276687]: [989.0716ms] [989.0716ms] END
I0918 19:29:59.630940       1 trace.go:205] Trace[1310876787]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints (18-Sep-2022 19:29:58.569) (total time: 1061ms):
Trace[1310876787]: ---"Txn call finished" err:<nil> 1058ms (19:29:59.630)
Trace[1310876787]: [1.0613053s] [1.0613053s] END
{"level":"warn","ts":"2022-09-18T19:33:01.674Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002fe0540/127.0.0.1:2379","attempt":0,"error":"rpc error: code = Unknown desc = context deadline exceeded"}
I0918 19:33:03.166073       1 trace.go:205] Trace[1791011415]: "Get" url:/api/v1/namespaces/kube-system,user-agent:kube-apiserver/v1.25.0 (linux/amd64) kubernetes/a866cbe,audit-id:3b59fd64-7bcf-47ed-867f-c106b3a7f66d,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (18-Sep-2022 19:32:59.652) (total time: 3488ms):
Trace[1791011415]: ---"About to write a response" 3488ms (19:33:03.165)
Trace[1791011415]: [3.4882783s] [3.4882783s] END
I0918 19:33:03.166394       1 trace.go:205] Trace[1179363126]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:a15883e0-8f29-4df0-8955-53691b9b6c9b,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (18-Sep-2022 19:33:01.082) (total time: 2059ms):
Trace[1179363126]: ---"About to write a response" 2058ms (19:33:03.166)
Trace[1179363126]: [2.0590326s] [2.0590326s] END
I0918 19:42:02.050543       1 alloc.go:327] "allocated clusterIPs" service="default/spring-boot-k8s" clusterIPs=map[IPv4:10.106.167.92]
I0918 19:46:52.061208       1 alloc.go:327] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs=map[IPv4:10.102.164.109]
I0918 19:46:52.129099       1 alloc.go:327] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs=map[IPv4:10.102.193.133]
I0918 19:48:39.536831       1 trace.go:205] Trace[1174352508]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints (18-Sep-2022 19:48:38.634) (total time: 902ms):
Trace[1174352508]: ---"Txn call finished" err:<nil> 899ms (19:48:39.536)
Trace[1174352508]: [902.2701ms] [902.2701ms] END

* 
* ==> kube-controller-manager [aa89259e717f] <==
* I0918 18:19:04.741391       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
I0918 18:19:04.741776       1 taint_manager.go:209] "Sending events to api server"
I0918 18:19:04.742772       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0918 18:19:04.743628       1 range_allocator.go:367] Set node minikube PodCIDR to [10.244.0.0/24]
I0918 18:19:04.745337       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0918 18:19:04.750597       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I0918 18:19:04.756046       1 shared_informer.go:262] Caches are synced for daemon sets
I0918 18:19:04.756126       1 shared_informer.go:262] Caches are synced for cronjob
I0918 18:19:04.757470       1 shared_informer.go:262] Caches are synced for deployment
I0918 18:19:04.758362       1 shared_informer.go:262] Caches are synced for expand
I0918 18:19:04.758481       1 shared_informer.go:262] Caches are synced for TTL after finished
I0918 18:19:04.763851       1 shared_informer.go:262] Caches are synced for ReplicaSet
I0918 18:19:04.765671       1 shared_informer.go:262] Caches are synced for GC
I0918 18:19:04.766937       1 shared_informer.go:262] Caches are synced for attach detach
I0918 18:19:04.775538       1 shared_informer.go:262] Caches are synced for job
I0918 18:19:04.822509       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I0918 18:19:04.822612       1 shared_informer.go:262] Caches are synced for stateful set
I0918 18:19:04.822653       1 shared_informer.go:262] Caches are synced for namespace
I0918 18:19:04.822527       1 shared_informer.go:262] Caches are synced for endpoint_slice
I0918 18:19:04.823252       1 shared_informer.go:262] Caches are synced for crt configmap
I0918 18:19:04.823385       1 shared_informer.go:262] Caches are synced for PVC protection
I0918 18:19:04.847361       1 shared_informer.go:262] Caches are synced for ReplicationController
I0918 18:19:04.856126       1 shared_informer.go:262] Caches are synced for HPA
I0918 18:19:04.856146       1 shared_informer.go:262] Caches are synced for disruption
I0918 18:19:04.872736       1 shared_informer.go:262] Caches are synced for resource quota
I0918 18:19:04.876070       1 shared_informer.go:262] Caches are synced for resource quota
I0918 18:19:05.284576       1 shared_informer.go:262] Caches are synced for garbage collector
I0918 18:19:05.284661       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0918 18:19:05.346253       1 shared_informer.go:262] Caches are synced for garbage collector
I0918 18:19:05.481010       1 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-565d847f94 to 1"
I0918 18:19:05.548970       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-hn5s9"
I0918 18:19:05.776197       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-565d847f94-47slz"
I0918 18:37:59.894077       1 event.go:294] "Event occurred" object="default/spring-boot-k8s" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set spring-boot-k8s-7c9884547b to 1"
I0918 18:37:59.929519       1 event.go:294] "Event occurred" object="default/spring-boot-k8s-7c9884547b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: spring-boot-k8s-7c9884547b-8dtj6"
I0918 18:50:29.489095       1 event.go:294] "Event occurred" object="default/spring-boot-k8s" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set spring-boot-k8s-7c9884547b to 1"
I0918 18:50:29.521989       1 event.go:294] "Event occurred" object="default/spring-boot-k8s-7c9884547b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: spring-boot-k8s-7c9884547b-2fdtd"
I0918 19:39:02.187190       1 event.go:294] "Event occurred" object="default/spring-boot-k8s" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set spring-boot-k8s-7c9884547b to 1"
I0918 19:39:02.219237       1 event.go:294] "Event occurred" object="default/spring-boot-k8s-7c9884547b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: spring-boot-k8s-7c9884547b-xfvm9"
I0918 19:46:51.474901       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dashboard-metrics-scraper-b74747df5 to 1"
I0918 19:46:51.545067       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-b74747df5-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0918 19:46:51.546542       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set kubernetes-dashboard-54596f475f to 1"
E0918 19:46:51.587671       1 replica_set.go:550] sync "kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" failed with pods "dashboard-metrics-scraper-b74747df5-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0918 19:46:51.626803       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-54596f475f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-54596f475f-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0918 19:46:51.670029       1 replica_set.go:550] sync "kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" failed with pods "dashboard-metrics-scraper-b74747df5-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0918 19:46:51.671765       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-b74747df5-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0918 19:46:51.737287       1 replica_set.go:550] sync "kubernetes-dashboard/kubernetes-dashboard-54596f475f" failed with pods "kubernetes-dashboard-54596f475f-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
E0918 19:46:51.750575       1 replica_set.go:550] sync "kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" failed with pods "dashboard-metrics-scraper-b74747df5-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0918 19:46:51.751118       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-b74747df5-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0918 19:46:51.758154       1 replica_set.go:550] sync "kubernetes-dashboard/kubernetes-dashboard-54596f475f" failed with pods "kubernetes-dashboard-54596f475f-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0918 19:46:51.758516       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-54596f475f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-54596f475f-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0918 19:46:51.768088       1 replica_set.go:550] sync "kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" failed with pods "dashboard-metrics-scraper-b74747df5-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0918 19:46:51.768200       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-b74747df5-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0918 19:46:51.773701       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-54596f475f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-54596f475f-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0918 19:46:51.773810       1 replica_set.go:550] sync "kubernetes-dashboard/kubernetes-dashboard-54596f475f" failed with pods "kubernetes-dashboard-54596f475f-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
E0918 19:46:51.822255       1 replica_set.go:550] sync "kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" failed with pods "dashboard-metrics-scraper-b74747df5-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0918 19:46:51.822363       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-b74747df5-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E0918 19:46:51.825968       1 replica_set.go:550] sync "kubernetes-dashboard/kubernetes-dashboard-54596f475f" failed with pods "kubernetes-dashboard-54596f475f-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0918 19:46:51.826150       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-54596f475f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-54596f475f-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0918 19:46:51.844054       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-54596f475f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-54596f475f-g85tw"
I0918 19:46:51.936886       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-b74747df5-m9rj4"

* 
* ==> kube-proxy [182f7b397fe9] <==
* E0918 18:19:06.777832       1 proxier.go:656] "Failed to read builtin modules file, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" err="open /lib/modules/5.10.16.3-microsoft-standard-WSL2/modules.builtin: no such file or directory" filePath="/lib/modules/5.10.16.3-microsoft-standard-WSL2/modules.builtin"
I0918 18:19:06.782380       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I0918 18:19:06.788306       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I0918 18:19:06.825490       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I0918 18:19:06.829240       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I0918 18:19:06.832521       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="nf_conntrack"
I0918 18:19:06.850443       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0918 18:19:06.850523       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0918 18:19:06.850577       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0918 18:19:06.954820       1 server_others.go:206] "Using iptables Proxier"
I0918 18:19:06.954911       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0918 18:19:06.954941       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0918 18:19:06.954975       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0918 18:19:06.955020       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0918 18:19:06.955489       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0918 18:19:06.955898       1 server.go:661] "Version info" version="v1.25.0"
I0918 18:19:06.955929       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0918 18:19:06.960255       1 config.go:317] "Starting service config controller"
I0918 18:19:06.961450       1 shared_informer.go:255] Waiting for caches to sync for service config
I0918 18:19:06.960578       1 config.go:226] "Starting endpoint slice config controller"
I0918 18:19:06.962048       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0918 18:19:06.960814       1 config.go:444] "Starting node config controller"
I0918 18:19:06.962272       1 shared_informer.go:255] Waiting for caches to sync for node config
I0918 18:19:07.063572       1 shared_informer.go:262] Caches are synced for node config
I0918 18:19:07.063572       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0918 18:19:07.065474       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-scheduler [5e524cf110cf] <==
* W0918 18:18:48.640228       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0918 18:18:48.640251       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W0918 18:18:48.640266       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0918 18:18:48.741738       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.0"
I0918 18:18:48.741828       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0918 18:18:48.754014       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0918 18:18:48.754419       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0918 18:18:48.754681       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0918 18:18:48.754861       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0918 18:18:48.822433       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0918 18:18:48.822719       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0918 18:18:48.823859       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0918 18:18:48.824066       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0918 18:18:48.824858       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0918 18:18:48.825029       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0918 18:18:48.825820       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0918 18:18:48.825983       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0918 18:18:48.826543       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0918 18:18:48.826702       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0918 18:18:48.825391       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0918 18:18:48.827669       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0918 18:18:48.828544       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0918 18:18:48.828614       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0918 18:18:48.828889       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0918 18:18:48.828940       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0918 18:18:48.829235       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0918 18:18:48.829752       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0918 18:18:48.829811       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0918 18:18:48.829760       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0918 18:18:48.829512       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0918 18:18:48.829903       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0918 18:18:48.829918       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0918 18:18:48.829932       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0918 18:18:48.829612       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0918 18:18:48.829970       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0918 18:18:48.829694       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0918 18:18:48.830005       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0918 18:18:48.829489       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0918 18:18:48.830055       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0918 18:18:49.646628       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0918 18:18:49.646715       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0918 18:18:49.821175       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0918 18:18:49.821317       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0918 18:18:49.861681       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0918 18:18:49.861748       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0918 18:18:49.920633       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0918 18:18:49.920940       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0918 18:18:49.956025       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0918 18:18:49.956297       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0918 18:18:49.985277       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0918 18:18:49.985498       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0918 18:18:50.019845       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0918 18:18:50.019959       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0918 18:18:50.019845       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0918 18:18:50.020040       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0918 18:18:50.091331       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0918 18:18:50.091418       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0918 18:18:50.097866       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0918 18:18:50.097962       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I0918 18:18:52.655069       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Sun 2022-09-18 18:18:06 UTC, end at Sun 2022-09-18 19:52:17 UTC. --
Sep 18 19:35:14 minikube kubelet[2025]: I0918 19:35:14.625757    2025 scope.go:115] "RemoveContainer" containerID="956026652d8bfa0803d546b671f831064eed2a0c111dbd7129850d2cea9715b9"
Sep 18 19:35:14 minikube kubelet[2025]: E0918 19:35:14.626313    2025 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-k8s-demo\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=springboot-k8s-demo pod=spring-boot-k8s-7c9884547b-2fdtd_default(a6b39507-a8c4-4b52-ba3f-cf0754720bad)\"" pod="default/spring-boot-k8s-7c9884547b-2fdtd" podUID=a6b39507-a8c4-4b52-ba3f-cf0754720bad
Sep 18 19:35:25 minikube kubelet[2025]: I0918 19:35:25.626122    2025 scope.go:115] "RemoveContainer" containerID="956026652d8bfa0803d546b671f831064eed2a0c111dbd7129850d2cea9715b9"
Sep 18 19:35:25 minikube kubelet[2025]: E0918 19:35:25.626611    2025 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-k8s-demo\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=springboot-k8s-demo pod=spring-boot-k8s-7c9884547b-2fdtd_default(a6b39507-a8c4-4b52-ba3f-cf0754720bad)\"" pod="default/spring-boot-k8s-7c9884547b-2fdtd" podUID=a6b39507-a8c4-4b52-ba3f-cf0754720bad
Sep 18 19:35:40 minikube kubelet[2025]: I0918 19:35:40.626792    2025 scope.go:115] "RemoveContainer" containerID="956026652d8bfa0803d546b671f831064eed2a0c111dbd7129850d2cea9715b9"
Sep 18 19:35:40 minikube kubelet[2025]: E0918 19:35:40.627314    2025 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-k8s-demo\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=springboot-k8s-demo pod=spring-boot-k8s-7c9884547b-2fdtd_default(a6b39507-a8c4-4b52-ba3f-cf0754720bad)\"" pod="default/spring-boot-k8s-7c9884547b-2fdtd" podUID=a6b39507-a8c4-4b52-ba3f-cf0754720bad
Sep 18 19:35:51 minikube kubelet[2025]: I0918 19:35:51.626146    2025 scope.go:115] "RemoveContainer" containerID="956026652d8bfa0803d546b671f831064eed2a0c111dbd7129850d2cea9715b9"
Sep 18 19:35:51 minikube kubelet[2025]: E0918 19:35:51.626661    2025 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-k8s-demo\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=springboot-k8s-demo pod=spring-boot-k8s-7c9884547b-2fdtd_default(a6b39507-a8c4-4b52-ba3f-cf0754720bad)\"" pod="default/spring-boot-k8s-7c9884547b-2fdtd" podUID=a6b39507-a8c4-4b52-ba3f-cf0754720bad
Sep 18 19:36:03 minikube kubelet[2025]: I0918 19:36:03.681497    2025 scope.go:115] "RemoveContainer" containerID="956026652d8bfa0803d546b671f831064eed2a0c111dbd7129850d2cea9715b9"
Sep 18 19:36:03 minikube kubelet[2025]: I0918 19:36:03.743881    2025 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-wwcn2\" (UniqueName: \"kubernetes.io/projected/a6b39507-a8c4-4b52-ba3f-cf0754720bad-kube-api-access-wwcn2\") pod \"a6b39507-a8c4-4b52-ba3f-cf0754720bad\" (UID: \"a6b39507-a8c4-4b52-ba3f-cf0754720bad\") "
Sep 18 19:36:03 minikube kubelet[2025]: I0918 19:36:03.748715    2025 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/a6b39507-a8c4-4b52-ba3f-cf0754720bad-kube-api-access-wwcn2" (OuterVolumeSpecName: "kube-api-access-wwcn2") pod "a6b39507-a8c4-4b52-ba3f-cf0754720bad" (UID: "a6b39507-a8c4-4b52-ba3f-cf0754720bad"). InnerVolumeSpecName "kube-api-access-wwcn2". PluginName "kubernetes.io/projected", VolumeGidValue ""
Sep 18 19:36:03 minikube kubelet[2025]: I0918 19:36:03.845062    2025 reconciler.go:399] "Volume detached for volume \"kube-api-access-wwcn2\" (UniqueName: \"kubernetes.io/projected/a6b39507-a8c4-4b52-ba3f-cf0754720bad-kube-api-access-wwcn2\") on node \"minikube\" DevicePath \"\""
Sep 18 19:36:05 minikube kubelet[2025]: I0918 19:36:05.646962    2025 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=a6b39507-a8c4-4b52-ba3f-cf0754720bad path="/var/lib/kubelet/pods/a6b39507-a8c4-4b52-ba3f-cf0754720bad/volumes"
Sep 18 19:38:53 minikube kubelet[2025]: W0918 19:38:53.867560    2025 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234243    2025 topology_manager.go:205] "Topology Admit Handler"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234399    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234432    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234449    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234466    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="4fb7c7e8-f548-4d94-87b4-147addf0a071" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234482    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234498    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234556    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234579    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234596    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="4fb7c7e8-f548-4d94-87b4-147addf0a071" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234611    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234631    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="4fb7c7e8-f548-4d94-87b4-147addf0a071" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234648    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="4fb7c7e8-f548-4d94-87b4-147addf0a071" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234663    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234677    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: E0918 19:39:02.234693    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234751    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234770    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234784    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234798    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234812    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="4fb7c7e8-f548-4d94-87b4-147addf0a071" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234829    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234843    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234859    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234871    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234889    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.234904    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:39:02 minikube kubelet[2025]: I0918 19:39:02.431187    2025 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kxlhn\" (UniqueName: \"kubernetes.io/projected/49af2a23-19c7-48f2-ae89-fb84f6be5286-kube-api-access-kxlhn\") pod \"spring-boot-k8s-7c9884547b-xfvm9\" (UID: \"49af2a23-19c7-48f2-ae89-fb84f6be5286\") " pod="default/spring-boot-k8s-7c9884547b-xfvm9"
Sep 18 19:43:53 minikube kubelet[2025]: W0918 19:43:53.867010    2025 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.861285    2025 topology_manager.go:205] "Topology Admit Handler"
Sep 18 19:46:51 minikube kubelet[2025]: E0918 19:46:51.863550    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:46:51 minikube kubelet[2025]: E0918 19:46:51.863685    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:46:51 minikube kubelet[2025]: E0918 19:46:51.863720    2025 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.863979    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="4fb7c7e8-f548-4d94-87b4-147addf0a071" containerName="springboot-k8s-demo"
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.864014    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.864030    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.864047    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.864061    2025 memory_manager.go:345] "RemoveStaleState removing state" podUID="a6b39507-a8c4-4b52-ba3f-cf0754720bad" containerName="springboot-k8s-demo"
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.949857    2025 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/165ae64d-f002-48b7-b98a-a8bdd4c8b8a5-tmp-volume\") pod \"kubernetes-dashboard-54596f475f-g85tw\" (UID: \"165ae64d-f002-48b7-b98a-a8bdd4c8b8a5\") " pod="kubernetes-dashboard/kubernetes-dashboard-54596f475f-g85tw"
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.950029    2025 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fl6g5\" (UniqueName: \"kubernetes.io/projected/165ae64d-f002-48b7-b98a-a8bdd4c8b8a5-kube-api-access-fl6g5\") pod \"kubernetes-dashboard-54596f475f-g85tw\" (UID: \"165ae64d-f002-48b7-b98a-a8bdd4c8b8a5\") " pod="kubernetes-dashboard/kubernetes-dashboard-54596f475f-g85tw"
Sep 18 19:46:51 minikube kubelet[2025]: I0918 19:46:51.954274    2025 topology_manager.go:205] "Topology Admit Handler"
Sep 18 19:46:52 minikube kubelet[2025]: I0918 19:46:52.051027    2025 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r4tpn\" (UniqueName: \"kubernetes.io/projected/7db3b53b-6942-42ce-8b66-698d3b441c7c-kube-api-access-r4tpn\") pod \"dashboard-metrics-scraper-b74747df5-m9rj4\" (UID: \"7db3b53b-6942-42ce-8b66-698d3b441c7c\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5-m9rj4"
Sep 18 19:46:52 minikube kubelet[2025]: I0918 19:46:52.051884    2025 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/7db3b53b-6942-42ce-8b66-698d3b441c7c-tmp-volume\") pod \"dashboard-metrics-scraper-b74747df5-m9rj4\" (UID: \"7db3b53b-6942-42ce-8b66-698d3b441c7c\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5-m9rj4"
Sep 18 19:46:53 minikube kubelet[2025]: I0918 19:46:53.870851    2025 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="9f2237d7bfb7eafa2b59eb30caed3498f01a461df46abba0c3099773a6eda501"
Sep 18 19:46:53 minikube kubelet[2025]: I0918 19:46:53.897563    2025 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="bf0b4efb96ea32342e6ada61e1650bd71ea80d7910892ca7c6dedebea6441c34"
Sep 18 19:48:53 minikube kubelet[2025]: W0918 19:48:53.868334    2025 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> kubernetes-dashboard [db4f909be2f6] <==
* 2022/09/18 19:50:48 [2022-09-18T19:50:48Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:48 received 0 resources from sidecar instead of 1
2022/09/18 19:50:48 [2022-09-18T19:50:48Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:48 received 0 resources from sidecar instead of 1
2022/09/18 19:50:48 Getting pod metrics
2022/09/18 19:50:48 received 0 resources from sidecar instead of 1
2022/09/18 19:50:48 received 0 resources from sidecar instead of 1
2022/09/18 19:50:48 Skipping metric because of error: Metric label not set.
2022/09/18 19:50:48 Skipping metric because of error: Metric label not set.
2022/09/18 19:50:48 [2022-09-18T19:50:48Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/09/18 19:50:51 Getting list of all cron jobs in the cluster
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 404 status code
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/09/18 19:50:51 Getting list of all deployments in the cluster
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/09/18 19:50:51 Getting list of all jobs in the cluster
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/09/18 19:50:51 Getting list of all pods in the cluster
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/09/18 19:50:51 Getting list of all replica sets in the cluster
2022/09/18 19:50:51 received 0 resources from sidecar instead of 1
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/09/18 19:50:51 Getting list of all replication controllers in the cluster
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/09/18 19:50:51 Getting list of all pet sets in the cluster
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 received 0 resources from sidecar instead of 1
2022/09/18 19:50:51 received 0 resources from sidecar instead of 1
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 received 0 resources from sidecar instead of 1
2022/09/18 19:50:51 Getting pod metrics
2022/09/18 19:50:51 received 0 resources from sidecar instead of 1
2022/09/18 19:50:51 received 0 resources from sidecar instead of 1
2022/09/18 19:50:51 received 0 resources from sidecar instead of 1
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:51 received 0 resources from sidecar instead of 1
2022/09/18 19:50:51 Skipping metric because of error: Metric label not set.
2022/09/18 19:50:51 Skipping metric because of error: Metric label not set.
2022/09/18 19:50:51 [2022-09-18T19:50:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:52 [2022-09-18T19:50:52Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/09/18 19:50:52 Getting list of namespaces
2022/09/18 19:50:52 [2022-09-18T19:50:52Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:50:57 [2022-09-18T19:50:57Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/09/18 19:50:57 Getting list of namespaces
2022/09/18 19:50:57 [2022-09-18T19:50:57Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:51:02 [2022-09-18T19:51:02Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/09/18 19:51:02 Getting list of namespaces
2022/09/18 19:51:02 [2022-09-18T19:51:02Z] Outcoming response to 127.0.0.1 with 200 status code
2022/09/18 19:51:03 [2022-09-18T19:51:03Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/09/18 19:51:03 Getting list of namespaces
2022/09/18 19:51:03 [2022-09-18T19:51:03Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [a0cded536fdb] <==
* I0918 18:19:29.728289       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0918 18:19:29.744637       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0918 18:19:29.744748       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0918 18:19:29.758446       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0918 18:19:29.758683       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_1ccf55a1-9825-43b4-b8a0-6bbd19764cc9!
I0918 18:19:29.758559       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"f6c47131-757a-4703-acdc-bdfea420c72a", APIVersion:"v1", ResourceVersion:"388", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_1ccf55a1-9825-43b4-b8a0-6bbd19764cc9 became leader
I0918 18:19:29.860403       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_1ccf55a1-9825-43b4-b8a0-6bbd19764cc9!

* 
* ==> storage-provisioner [c74a85f27d66] <==
* I0918 18:19:07.635586       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0918 18:19:28.674735       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

